{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8c37b9",
   "metadata": {
    "papermill": {
     "duration": 0.007589,
     "end_time": "2024-12-25T16:20:47.790365",
     "exception": false,
     "start_time": "2024-12-25T16:20:47.782776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tổng quan: Phát hiện đối tượng với Faster R-CNN\n",
    "\n",
    "## Mục tiêu\n",
    "Phát triển một mô hình phát hiện đối tượng để vẽ **bounding box** xung quanh các đối tượng trong ảnh và gán nhãn tương ứng. Trong bài toán này, thuật toán **Faster R-CNN** được sử dụng với các thành phần chính:\n",
    "\n",
    "1. **Mạng Backbone**: Học các đặc trưng (features) của ảnh.\n",
    "2. **Region Proposal Network (RPN)**: Tìm các anchor ứng viên, đóng vai trò là đầu vào cho bước tiếp theo.\n",
    "3. **RoI Pooling**: Kết hợp đầu ra từ RPN và các đặc trưng của ảnh để xác định bounding box và nhãn tương ứng.\n",
    "\n",
    "### Kiến trúc Faster R-CNN\n",
    "Dưới đây là sơ đồ minh họa kiến trúc của Faster R-CNN:\n",
    "\n",
    "![Faster R-CNN Architecture](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSRW1T3Csvlu2VxcPI_KW_d5e-_5aPG-QLEwg&s)\n",
    "\n",
    "---\n",
    "\n",
    "## Dữ liệu\n",
    "- **Tổng số ảnh**: 7600 ảnh.\n",
    "- **Số nhãn**: 6 nhãn (multi-class object detection).\n",
    "- **Kích thước ảnh**: 640x640 pixel.\n",
    "- **Phân chia dữ liệu**:\n",
    "  - **Train**: Toàn bộ tập ảnh dùng để huấn luyện mô hình.\n",
    "  - **Test**: 20 ảnh được lựa chọn (kiểm tra thủ công bằng mắt thường để đánh giá chất lượng).\n",
    "\n",
    "---\n",
    "\n",
    "## Phương pháp\n",
    "Xây dựng mô hình theo cách tiếp cận \"từ dưới lên trên,\" tức là đi từ từng thành phần cơ bản của thuật toán (RPN, Backbone, RoI Pooling) đến hệ thống hoàn chỉnh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f3b12",
   "metadata": {
    "papermill": {
     "duration": 0.006354,
     "end_time": "2024-12-25T16:20:47.803414",
     "exception": false,
     "start_time": "2024-12-25T16:20:47.797060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Xây dựng mô hình Faster R-CNN\n",
    "\n",
    "## 1. Xây dựng mô hình ResNet18\n",
    "Mục tiêu là thiết lập mô hình trích xuất đặc trưng của ảnh. Trong bài toán này, sử dụng **ResNet18** làm mạng backbone. \n",
    "\n",
    "### Cách thực hiện:\n",
    "- Sử dụng mô hình **ResNet18** đã được huấn luyện trước (pre-trained).\n",
    "- Loại bỏ các lớp cuối cùng:\n",
    "  - **Lớp avgpool** (average pooling).\n",
    "  - **Lớp Linear** (fully connected layer).\n",
    "\n",
    "### Đầu vào và đầu ra:\n",
    "- **Input**: \n",
    "  - Kích thước: `(batch_size=4, channels=3, height=640, width=640)` \n",
    "  - Tương ứng với batch gồm 4 ảnh, mỗi ảnh có 3 kênh màu RGB, kích thước 640x640.\n",
    "- **Output**: \n",
    "  - Kích thước: `(batch_size=4, channels=512, height=20, width=20)`\n",
    "  - Gồm 512 kênh đặc trưng, với mỗi kênh có kích thước 20x20.\n",
    "\n",
    "Kết quả đầu ra này sẽ được sử dụng làm input cho các bước tiếp theo trong pipeline của Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce6c4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:47.817556Z",
     "iopub.status.busy": "2024-12-25T16:20:47.817172Z",
     "iopub.status.idle": "2024-12-25T16:20:52.212182Z",
     "shell.execute_reply": "2024-12-25T16:20:52.211491Z"
    },
    "papermill": {
     "duration": 4.403986,
     "end_time": "2024-12-25T16:20:52.213865",
     "exception": false,
     "start_time": "2024-12-25T16:20:47.809879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "class BackBoneResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        resnet18 = models.resnet18(weights=weights)\n",
    "\n",
    "        children = [child for child in resnet18.children()]\n",
    "\n",
    "        self.model = nn.Sequential(*children[:-2])\n",
    "        # self.model = resnet18.features[:-1]\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out = self.model(X)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f589d",
   "metadata": {
    "papermill": {
     "duration": 0.006861,
     "end_time": "2024-12-25T16:20:52.228269",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.221408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Xây dựng Region Proposal Network (RPN)\n",
    "\n",
    "### Định nghĩa một số hàm hỗ trợ trong RPN\n",
    "\n",
    "#### Hàm `get_iou`\n",
    "Hàm này tính tỷ lệ chồng lấp (IoU - Intersection over Union) giữa các anchor. Đây là một bước quan trọng trong việc xác định chất lượng của các anchor được đề xuất bởi RPN.\n",
    "\n",
    "### Công thức:\n",
    "$$\n",
    "IoU = \\frac{\\text{Diện tích phần chồng lấp (overlap)}}{\\text{Diện tích hợp nhất (union)}}\n",
    "$$\n",
    "\n",
    "### Ý nghĩa:\n",
    "- **Diện tích chồng lấp (overlap)**: Phần giao nhau giữa hai anchor trên mặt phẳng.\n",
    "- **Diện tích hợp nhất (union)**: Tổng diện tích của hai anchor trừ đi phần diện tích giao nhau.\n",
    "\n",
    "### Ứng dụng:\n",
    "- Dùng để đánh giá các anchor:\n",
    "  - Anchor nào tốt (có IoU cao) sẽ được chọn làm ứng viên cho bước tiếp theo.\n",
    "  - Anchor nào có IoU thấp sẽ bị loại bỏ.\n",
    "\n",
    "Hàm `get_iou` sẽ là một phần quan trọng trong quá trình sinh ra các anchor và chọn lọc chúng trong pipeline của Region Proposal Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1a2c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.243032Z",
     "iopub.status.busy": "2024-12-25T16:20:52.242641Z",
     "iopub.status.idle": "2024-12-25T16:20:52.248535Z",
     "shell.execute_reply": "2024-12-25T16:20:52.247708Z"
    },
    "papermill": {
     "duration": 0.014507,
     "end_time": "2024-12-25T16:20:52.249828",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.235321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_iou(boxes1, boxes2):\n",
    "    r\"\"\"\n",
    "    IOU between two sets of boxes\n",
    "    :param boxes1: (Tensor of shape N x 4)\n",
    "    :param boxes2: (Tensor of shape M x 4)\n",
    "    :return: IOU matrix of shape N x M\n",
    "    \"\"\"\n",
    "    # Area of boxes (x2-x1)*(y2-y1)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])  # (N,)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])  # (M,)\n",
    "    \n",
    "    # Get top left x1,y1 coordinate\n",
    "    x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])  # (N, M)\n",
    "    y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])  # (N, M)\n",
    "    \n",
    "    # Get bottom right x2,y2 coordinate\n",
    "    x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])  # (N, M)\n",
    "    y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])  # (N, M)\n",
    "    \n",
    "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)  # (N, M)\n",
    "    union = area1[:, None] + area2 - intersection_area  # (N, M)\n",
    "    iou = intersection_area / union  # (N, M)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44b598",
   "metadata": {
    "papermill": {
     "duration": 0.006269,
     "end_time": "2024-12-25T16:20:52.262742",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.256473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Hàm `clamp_boxes_to_image_boundary`\n",
    "\n",
    "Hàm này đảm bảo rằng các tọa độ của các hộp giới hạn (bounding boxes) nằm hoàn toàn trong biên giới của hình ảnh. Nếu các hộp vượt ra ngoài phạm vi hình ảnh, các tọa độ sẽ được điều chỉnh sao cho phù hợp với kích thước thực tế của hình ảnh.\n",
    "\n",
    "### Đầu vào:\n",
    "- **boxes**: Tensor có dạng `(N, 4)`, với mỗi hàng biểu diễn một hộp giới hạn dưới dạng `(x_min, y_min, x_max, y_max)`.\n",
    "- **image_shape**: Kích thước của hình ảnh dưới dạng tuple `(height, width)`.\n",
    "\n",
    "### Đầu ra:\n",
    "- **boxes**: Tensor với các tọa độ đã được điều chỉnh sao cho chúng nằm trong biên giới của hình ảnh.\n",
    "\n",
    "### Mô tả:\n",
    "- Hàm này điều chỉnh các giá trị của `x_min`, `y_min`, `x_max`, `y_max` sao cho chúng không vượt qua phạm vi của hình ảnh. Các giá trị sẽ được giới hạn trong phạm vi từ 0 đến chiều cao (height) và chiều rộng (width) của hình ảnh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17e153d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.276880Z",
     "iopub.status.busy": "2024-12-25T16:20:52.276569Z",
     "iopub.status.idle": "2024-12-25T16:20:52.281526Z",
     "shell.execute_reply": "2024-12-25T16:20:52.280705Z"
    },
    "papermill": {
     "duration": 0.013477,
     "end_time": "2024-12-25T16:20:52.282854",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.269377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
    "    boxes_x1 = boxes[..., 0]\n",
    "    boxes_y1 = boxes[..., 1]\n",
    "    boxes_x2 = boxes[..., 2]\n",
    "    boxes_y2 = boxes[..., 3]\n",
    "    height, width = image_shape[-2:]\n",
    "    boxes_x1 = boxes_x1.clamp(min=0, max=width)\n",
    "    boxes_x2 = boxes_x2.clamp(min=0, max=width)\n",
    "    boxes_y1 = boxes_y1.clamp(min=0, max=height)\n",
    "    boxes_y2 = boxes_y2.clamp(min=0, max=height)\n",
    "    boxes = torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y2[..., None]),\n",
    "        dim=-1)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65450609",
   "metadata": {
    "papermill": {
     "duration": 0.006704,
     "end_time": "2024-12-25T16:20:52.296220",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.289516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Hàm `apply_regression_pred_to_anchors_or_proposals`\n",
    "\n",
    "**Mục đích**:  \n",
    "Hàm này có nhiệm vụ biến đổi các thông số dự đoán từ mô hình (dưới dạng `dx`, `dy`, `dw`, `dh`) cho các anchor hoặc proposal, thành các hộp dự đoán (predicted boxes).\n",
    "\n",
    "### Đầu vào:\n",
    "- **anchors** (hoặc **proposals**): Tensor có dạng `(N, 4)`, trong đó mỗi hàng biểu diễn một hộp giới hạn (anchor hoặc proposal) dưới dạng `(x_min, y_min, x_max, y_max)`.\n",
    "- **predictions**: Tensor có dạng `(N, 4)`, với mỗi hàng chứa các thông số dự đoán từ mô hình dưới dạng `(dx, dy, dw, dh)`.\n",
    "\n",
    "### Đầu ra:\n",
    "- **predicted_boxes**: Tensor có dạng `(N, 4)`, là các hộp giới hạn dự đoán (predicted boxes) được tính từ các thông số dự đoán và các anchor hoặc proposal ban đầu.\n",
    "\n",
    "### Mô tả:\n",
    "- Hàm này sử dụng các thông số dự đoán `dx`, `dy`, `dw`, `dh` để điều chỉnh các giá trị của các anchor hoặc proposal ban đầu, từ đó tạo ra các hộp giới hạn dự đoán. Quá trình này bao gồm việc áp dụng các phép toán hồi quy lên tọa độ của anchor hoặc proposal để tạo ra các predicted boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec52b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.310616Z",
     "iopub.status.busy": "2024-12-25T16:20:52.310337Z",
     "iopub.status.idle": "2024-12-25T16:20:52.316819Z",
     "shell.execute_reply": "2024-12-25T16:20:52.316015Z"
    },
    "papermill": {
     "duration": 0.015522,
     "end_time": "2024-12-25T16:20:52.318100",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.302578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Dựa trên dự đoán các tham số biến đổi box cho tất cả các anchor hoặc proposal đầu vào,\n",
    "    biến đổi chúng tương ứng để tạo ra các proposal hoặc box dự đoán.\n",
    "\n",
    "    :param box_transform_pred: Tensor có kích thước (num_anchors_or_proposals, num_classes, 4),\n",
    "                               chứa các dự đoán thay đổi box (dx, dy, dw, dh) cho từng anchor hoặc proposal.\n",
    "    :param anchors_or_proposals: Tensor có kích thước (num_anchors_or_proposals, 4),\n",
    "                                 chứa các giá trị của các anchor hoặc proposal dưới dạng (x1, y1, x2, y2).\n",
    "    :return pred_boxes: Tensor có kích thước (num_anchors_or_proposals, num_classes, 4),\n",
    "                        chứa các box dự đoán cho từng lớp (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Biến đổi lại kích thước của box_transform_pred\n",
    "    box_transform_pred = box_transform_pred.reshape(\n",
    "        box_transform_pred.size(0), -1, 4)\n",
    "\n",
    "    # Tính toán chiều rộng (w) và chiều cao (h) từ x1, y1, x2, y2 của anchor hoặc proposal\n",
    "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
    "    \n",
    "    # Lấy các giá trị dx, dy, dw, dh từ dự đoán\n",
    "    dx = box_transform_pred[..., 0]\n",
    "    dy = box_transform_pred[..., 1]\n",
    "    dw = box_transform_pred[..., 2]\n",
    "    dh = box_transform_pred[..., 3]\n",
    "    \n",
    "    # Giới hạn giá trị dw, dh để tránh đưa quá giá trị lớn vào hàm torch.exp()\n",
    "    dw = torch.clamp(dw, max=math.log(1000.0 / 16))\n",
    "    dh = torch.clamp(dh, max=math.log(1000.0 / 16))\n",
    "    \n",
    "    # Tính toán các giá trị trung tâm và kích thước của hộp dự đoán\n",
    "    pred_center_x = dx * w[:, None] + center_x[:, None]\n",
    "    pred_center_y = dy * h[:, None] + center_y[:, None]\n",
    "    pred_w = torch.exp(dw) * w[:, None]\n",
    "    pred_h = torch.exp(dh) * h[:, None]\n",
    "    \n",
    "    # Tính toán các tọa độ của hộp dự đoán (x1, y1, x2, y2)\n",
    "    pred_box_x1 = pred_center_x - 0.5 * pred_w\n",
    "    pred_box_y1 = pred_center_y - 0.5 * pred_h\n",
    "    pred_box_x2 = pred_center_x + 0.5 * pred_w\n",
    "    pred_box_y2 = pred_center_y + 0.5 * pred_h\n",
    "    \n",
    "    # Kết hợp các tọa độ của hộp dự đoán vào một tensor duy nhất\n",
    "    pred_boxes = torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2),\n",
    "        dim=2)\n",
    "    \n",
    "    return pred_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8841072",
   "metadata": {
    "papermill": {
     "duration": 0.006281,
     "end_time": "2024-12-25T16:20:52.331101",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.324820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Hàm `boxes_to_transformation_targets`\n",
    "\n",
    "**Mục đích**:  \n",
    "Hàm này chuyển đổi các tọa độ hộp (x1, y1, x2, y2) của các hộp ground truth và các anchor/proposal thành các giá trị biến đổi (tx, ty, tw, th) mà mô hình có thể sử dụng trong quá trình huấn luyện.\n",
    "\n",
    "### Cách tính các giá trị biến đổi:\n",
    "1. **tx: Sự thay đổi trung tâm theo trục x:**\n",
    "$$\n",
    "\\text{targets\\_dx} = \\frac{\\text{gt\\_center\\_x} - \\text{center\\_x}}{\\text{widths}}\n",
    "$$\n",
    "\n",
    "2. **ty: Sự thay đổi trung tâm theo trục y:**\n",
    "$$\n",
    "\\text{targets\\_dy} = \\frac{\\text{gt\\_center\\_y} - \\text{center\\_y}}{\\text{heights}}\n",
    "$$\n",
    "\n",
    "3. **tw: Sự thay đổi chiều rộng:**\n",
    "$$\n",
    "\\text{targets\\_dw} = \\log \\left( \\frac{\\text{gt\\_widths}}{\\text{widths}} \\right)\n",
    "$$\n",
    "\n",
    "4. **th: Sự thay đổi chiều cao:**\n",
    "$$\n",
    "\\text{targets\\_dh} = \\log \\left( \\frac{\\text{gt\\_heights}}{\\text{heights}} \\right)\n",
    "$$\n",
    "\n",
    "### Đầu vào:\n",
    "- **gt_boxes**: Tensor có dạng `(N, 4)`, với mỗi hàng là các tọa độ của hộp ground truth dưới dạng `(x1, y1, x2, y2)`.\n",
    "- **anchors**: Tensor có dạng `(N, 4)`, với mỗi hàng là các tọa độ của anchor/proposal dưới dạng `(x1, y1, x2, y2)`.\n",
    "\n",
    "### Đầu ra:\n",
    "- **tx, ty, tw, th**: Các giá trị biến đổi (dx, dy, dw, dh) được tính cho từng anchor/proposal so với hộp ground truth.\n",
    "\n",
    "### Mô tả:\n",
    "Hàm này giúp tính toán các giá trị biến đổi để huấn luyện mô hình phát hiện đối tượng. Các giá trị biến đổi này giúp mô hình có thể học cách chuyển đổi từ các anchor hoặc proposal ban đầu thành các hộp giới hạn chính xác hơn thông qua quá trình hồi quy trong quá trình huấn luyện.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48e0d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.345231Z",
     "iopub.status.busy": "2024-12-25T16:20:52.344933Z",
     "iopub.status.idle": "2024-12-25T16:20:52.350442Z",
     "shell.execute_reply": "2024-12-25T16:20:52.349640Z"
    },
    "papermill": {
     "duration": 0.013791,
     "end_time": "2024-12-25T16:20:52.351677",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.337886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
    "    r\"\"\"\n",
    "    Dựa trên các hộp ground truth và các anchor/proposal trong hình ảnh, hàm này tính toán\n",
    "    các giá trị tx, ty, tw, th (biến đổi tọa độ hộp) cho tất cả các anchor hoặc proposal.\n",
    "    \n",
    "    :param ground_truth_boxes: Tensor có kích thước (anchors_or_proposals_in_image, 4),\n",
    "                                chứa các giá trị tọa độ (x1, y1, x2, y2) của các hộp ground truth.\n",
    "    :param anchors_or_proposals: Tensor có kích thước (anchors_or_proposals_in_image, 4),\n",
    "                                 chứa các giá trị tọa độ (x1, y1, x2, y2) của các anchor hoặc proposal.\n",
    "    :return: regression_targets: Tensor có kích thước (anchors_or_proposals_in_image, 4),\n",
    "                                 chứa các giá trị tx, ty, tw, th (biến đổi tọa độ hộp) cho tất cả\n",
    "                                 các anchor/proposal.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tính toán center_x, center_y, w, h từ x1, y1, x2, y2 cho các anchors\n",
    "    widths = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    heights = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * widths\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * heights\n",
    "    \n",
    "    # Tính toán center_x, center_y, w, h từ x1, y1, x2, y2 cho các hộp ground truth\n",
    "    gt_widths = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths\n",
    "    gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights\n",
    "    \n",
    "    # Tính toán các giá trị tx, ty, tw, th từ sự khác biệt giữa ground truth và anchor/proposal\n",
    "    targets_dx = (gt_center_x - center_x) / widths\n",
    "    targets_dy = (gt_center_y - center_y) / heights\n",
    "    targets_dw = torch.log(gt_widths / widths)\n",
    "    targets_dh = torch.log(gt_heights / heights)\n",
    "    \n",
    "    # Gộp các giá trị tx, ty, tw, th thành một tensor\n",
    "    regression_targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)\n",
    "    \n",
    "    return regression_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1b30b",
   "metadata": {
    "papermill": {
     "duration": 0.006324,
     "end_time": "2024-12-25T16:20:52.364638",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.358314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Hàm `sample_positive_negative`\n",
    "\n",
    "**Mục đích**:  \n",
    "Hàm này chọn ngẫu nhiên một số lượng cụ thể các anchor box \"positive\" và \"negative\" từ các anchor box có sẵn, tạo ra các mặt nạ để xác định những anchor box này. Các anchor box được chọn sẽ được chuẩn bị cho quá trình huấn luyện của Region Proposal Network (RPN), giúp cân bằng giữa các anchor hợp lệ (chứa object) và các anchor không hợp lệ (không chứa object).\n",
    "\n",
    "### Cách thức hoạt động:\n",
    "1. **Positive Anchors**:  \n",
    "   Các anchor box được gắn nhãn \"positive\" nếu chúng có tỉ lệ IoU (Intersection over Union) lớn hơn một ngưỡng nhất định với các hộp ground truth.\n",
    "\n",
    "2. **Negative Anchors**:  \n",
    "   Các anchor box được gắn nhãn \"negative\" nếu chúng có tỉ lệ IoU nhỏ hơn một ngưỡng nhất định với các hộp ground truth.\n",
    "\n",
    "3. **Chọn ngẫu nhiên**:  \n",
    "   Sau khi phân loại các anchor box thành \"positive\" và \"negative\", hàm sẽ chọn một số lượng ngẫu nhiên các anchor box từ mỗi loại, nhằm đảm bảo sự cân bằng giữa các anchor hợp lệ và không hợp lệ.\n",
    "\n",
    "### Mô tả:\n",
    "Hàm `sample_positive_negative` giúp xác định các anchor box nào sẽ được sử dụng trong quá trình huấn luyện để tối ưu hóa mô hình. Việc chọn các anchor \"positive\" và \"negative\" giúp mô hình học cách phân biệt giữa các anchor chứa đối tượng và các anchor không chứa đối tượng, từ đó cải thiện độ chính xác của Region Proposal Network (RPN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a70b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.378461Z",
     "iopub.status.busy": "2024-12-25T16:20:52.378128Z",
     "iopub.status.idle": "2024-12-25T16:20:52.382974Z",
     "shell.execute_reply": "2024-12-25T16:20:52.382327Z"
    },
    "papermill": {
     "duration": 0.013155,
     "end_time": "2024-12-25T16:20:52.384120",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.370965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    # Sample positive và negative proposals\n",
    "    positive = torch.where(labels >= 1)[0]\n",
    "    negative = torch.where(labels == 0)[0]\n",
    "    num_pos = positive_count\n",
    "    num_pos = min(positive.numel(), num_pos)\n",
    "    num_neg = total_count - num_pos\n",
    "    num_neg = min(negative.numel(), num_neg)\n",
    "    perm_positive_idxs = torch.randperm(positive.numel(),\n",
    "                                        device=positive.device)[:num_pos]\n",
    "    perm_negative_idxs = torch.randperm(negative.numel(),\n",
    "                                        device=negative.device)[:num_neg]\n",
    "    pos_idxs = positive[perm_positive_idxs]\n",
    "    neg_idxs = negative[perm_negative_idxs]\n",
    "    sampled_pos_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_neg_idx_mask = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_pos_idx_mask[pos_idxs] = True\n",
    "    sampled_neg_idx_mask[neg_idxs] = True\n",
    "    return sampled_neg_idx_mask, sampled_pos_idx_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec2b5f",
   "metadata": {
    "papermill": {
     "duration": 0.00747,
     "end_time": "2024-12-25T16:20:52.398893",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.391423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Xây dựng Region Proposal Network (RPN)\n",
    "\n",
    "Tiếp theo, em sẽ xây dựng **Region Proposal Network (RPN)**, một phần quan trọng trong Faster R-CNN. RPN về cơ bản là một mạng sinh các anchor tại mỗi điểm của feature map, sau đó sử dụng các thuật toán như **Non-maximum Suppression (NMS)** để lọc ra top K các anchor tốt nhất.\n",
    "\n",
    "#### Các phương thức trong `RegionProposalNetwork`:\n",
    "\n",
    "1. **`generate_anchor`**:\n",
    "   - Mục đích: Tạo ra các anchor tại mỗi điểm trên feature map.\n",
    "   - Đầu vào:\n",
    "     - **image** (ban đầu) và **feature map**.\n",
    "   - Đầu ra: Tại mỗi vị trí trên feature map, có **K=9 anchors** với thông tin `(x, y)` là tọa độ tâm của anchor và `(h, w)` là kích thước của anchor.\n",
    "\n",
    "2. **`assign_targets_to_anchors`**:\n",
    "   - Mục đích: Gán nhãn và tọa độ cho từng anchor.\n",
    "   - Mỗi anchor sẽ được gán nhãn dựa trên tỉ lệ **Intersection over Union (IoU)** với ground truth box:\n",
    "     - **Foreground (label = 1)**: Anchor có IoU với ground truth box lớn hơn ngưỡng **high_iou_threshold**.\n",
    "     - **Background (label = 0)**: Anchor có IoU nhỏ hơn ngưỡng **low_iou_threshold**.\n",
    "     - **Ignored (label = -1)**: Anchor có IoU nằm giữa hai ngưỡng và bị bỏ qua trong huấn luyện.\n",
    "   - Đảm bảo mỗi ground truth box có ít nhất một anchor chất lượng cao được gán.\n",
    "\n",
    "3. **`filter_proposals`**:\n",
    "   - Mục đích: Sử dụng thuật toán **Non-Maximum Suppression (NMS)** để lọc ra **K proposal anchors** tốt nhất.\n",
    "\n",
    "4. **`forward`**:\n",
    "   - Các bước trong quá trình forward:\n",
    "     1. Gọi các lớp RPN để tạo dự đoán phân loại (classification) và biến đổi hộp giới hạn (bbox transformation) cho các anchors.\n",
    "     2. Sinh các anchors cho toàn bộ hình ảnh.\n",
    "     3. Biến đổi các anchors dựa trên dự đoán biến đổi hộp giới hạn để tạo ra các proposals.\n",
    "     4. Lọc các proposals.\n",
    "     5. Trong quá trình huấn luyện:\n",
    "        - Gán nhãn mục tiêu và hộp giới hạn ground truth cho từng anchor.\n",
    "        - Lấy mẫu các anchors dương (positive) và âm (negative).\n",
    "        - Tính toán tổn thất phân loại sử dụng các anchors dương/âm.\n",
    "        - Tính toán tổn thất định vị (localization loss) sử dụng các anchors dương.\n",
    "\n",
    "#### Mạng RPN:\n",
    "- **RPN conv layer**: Lớp convolution này không thay đổi số lượng kênh đầu vào nhưng giúp mô hình học được các đặc trưng cần thiết cho phân loại và hồi quy.\n",
    "- **Cls layer**: Lớp này thực hiện phân loại cho từng anchor trong feature map.\n",
    "- **BBox regression layer**: Dự đoán các tọa độ mới `(x1, y1, x2, y2)` cho mỗi anchor.\n",
    "\n",
    "### Loss function của RPN:\n",
    "Mục tiêu của RPN là học cách phân loại và điều chỉnh các anchor boxes sao cho chúng có thể khớp tốt với các đối tượng trong ảnh. Tổng tổn thất **RPN loss** là sự kết hợp của hai thành phần:\n",
    "\n",
    "1. **Classification Loss**: Tổn thất phân loại xác định xem anchor là foreground (1) hay background (0). Sử dụng **Binary Cross-Entropy Loss** cho phân loại.\n",
    "   \n",
    "   $$\n",
    "   L_{cls} = - \\frac{1}{N_{cls}} \\sum_{i} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "   $$\n",
    "\n",
    "   Trong đó:\n",
    "   - $ y_i $ là nhãn của anchor $ i $ (1 cho foreground, 0 cho background).\n",
    "   - $ p_i $ là xác suất mà anchor $ i $ được phân loại là foreground.\n",
    "\n",
    "2. **Bounding Box Regression Loss**: Tổn thất hồi quy xác định sự thay đổi giữa các anchor box và các ground truth box. Sử dụng **Smooth L1 Loss** cho tổn thất này.\n",
    "   \n",
    "   $$\n",
    "   L_{bbox} = \\frac{1}{N_{reg}} \\sum_{i} \\text{SmoothL1}(t_i - \\hat{t}_i)\n",
    "   $$\n",
    "\n",
    "   Trong đó:\n",
    "   - $ t_i $ là biến đổi hộp giới hạn cho anchor $i$.\n",
    "   - $\\hat{t}_i $ là dự đoán biến đổi hộp giới hạn từ mô hình.\n",
    "   - **Smooth L1 Loss** được tính như sau:\n",
    "\n",
    "   $$\n",
    "   \\text{SmoothL1}(x) =\n",
    "   \\begin{cases}\n",
    "   0.5x^2, & \\text{nếu } |x| < 1 \\\\\n",
    "   |x| - 0.5, & \\text{nếu } |x| \\geq 1\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "3. **Tổng tổn thất (RPN Loss)**: Tổng tổn thất là sự kết hợp của hai thành phần trên:\n",
    "   \n",
    "   $$\n",
    "   L_{rpn} = L_{cls} + \\lambda L_{bbox}\n",
    "   $$\n",
    "   \n",
    "   Trong đó:\n",
    "   - $\\lambda$ là một hệ số cân bằng giữa các thành phần phân loại và hồi quy.\n",
    "\n",
    "Tổn thất này sẽ được tối ưu hóa trong quá trình huấn luyện để cải thiện khả năng phát hiện đối tượng của RPN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e63674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.415675Z",
     "iopub.status.busy": "2024-12-25T16:20:52.415340Z",
     "iopub.status.idle": "2024-12-25T16:20:52.436865Z",
     "shell.execute_reply": "2024-12-25T16:20:52.435977Z"
    },
    "papermill": {
     "duration": 0.031736,
     "end_time": "2024-12-25T16:20:52.438413",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.406677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    r\"\"\"\n",
    "    RPN với các lớp sau trên feature map\n",
    "        1. Lớp đối lưu 3x3 theo sau là Relu\n",
    "        2. Chuyển đổi phân loại 1x1 với các kênh đầu ra num_anchors(num_scales x num_aspect_ratios)\n",
    "        3. Chuyển đổi phân loại 1x1 với 4 kênh đầu ra num_anchors\n",
    "\n",
    "    Việc phân loại được thực hiện thông qua một giá trị biểu thị xác suất của foreground\n",
    "    với sigmoid được áp dụng trong quá trình inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, scales, aspect_ratios, model_config):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.low_iou_threshold = model_config['rpn_bg_threshold']\n",
    "        self.high_iou_threshold = model_config['rpn_fg_threshold']\n",
    "        self.rpn_nms_threshold = model_config['rpn_nms_threshold']\n",
    "        self.rpn_batch_size = model_config['rpn_batch_size']\n",
    "        self.rpn_pos_count = int(model_config['rpn_pos_fraction'] * self.rpn_batch_size)\n",
    "        self.rpn_topk = model_config['rpn_train_topk'] if self.training else model_config['rpn_test_topk']\n",
    "        self.rpn_prenms_topk = model_config['rpn_train_prenms_topk'] if self.training \\\n",
    "            else model_config['rpn_test_prenms_topk']\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "        \n",
    "        # 3x3 conv layer\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 1x1 classification conv layer\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        \n",
    "        # 1x1 regression\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "        \n",
    "        for layer in [self.rpn_conv, self.cls_layer, self.bbox_reg_layer]:\n",
    "            torch.nn.init.normal_(layer.weight, std=0.01)\n",
    "            torch.nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def generate_anchors(self, image, feat): # pass the test\n",
    "        r\"\"\"\n",
    "        Phương thức để tạo neo. Đầu tiên chúng ta tạo ra một tập hợp các neo không có trọng tâm\n",
    "        bằng cách sử dụng thang đo và tỷ lệ khung hình được cung cấp.\n",
    "        Sau đó, tạo các giá trị dịch chuyển theo trục x, y cho tất cả các vị trí feature map.\n",
    "        Các điểm neo ở giữa bằng 0 được tạo ra sẽ được sao chép và dịch chuyển tương ứng\n",
    "        để tạo điểm neo cho tất cả các vị trí bản đồ đặc trưng.\n",
    "        Lưu ý rằng các điểm neo này được tạo sao cho tâm của chúng nằm ở góc trên bên trái của\n",
    "        ô bản đồ đặc trưng chứ không phải là trung tâm của ô feature map.\n",
    "        :param ảnh: tensor (N, C, H, W)\n",
    "        :param feat: (N, C_feat, H_feat, W_feat) tensor\n",
    "        :return: (H_feat * W_feat * num_anchors_per_location, 4)\n",
    "        \"\"\"\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "\n",
    "        # Calculate stride values for height and width\n",
    "        stride_h = (image_h // grid_h)\n",
    "        stride_w = (image_w // grid_w)\n",
    "\n",
    "        scales = torch.tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "\n",
    "        # Compute height and width ratios based on aspect ratios\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "\n",
    "        # Calculate anchor box widths and heights based on scales and aspect ratios\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).flatten()\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).flatten()\n",
    "\n",
    "        # Create base anchors centered at the origin\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "\n",
    "        # Compute shifts for x and y axes based on the feature map grid\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "\n",
    "        # Generate a meshgrid of shifts\n",
    "        shifts_y, shifts_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "\n",
    "        # Flatten the shifts\n",
    "        shifts = torch.stack((shifts_x.flatten(), shifts_y.flatten(),\n",
    "                            shifts_x.flatten(), shifts_y.flatten()), dim=1)\n",
    "\n",
    "        # Add shifts to base anchors to create all anchor boxes\n",
    "        anchors = (shifts[:, None, :] + base_anchors[None, :, :]).reshape(-1, 4)\n",
    "\n",
    "        return anchors\n",
    "    \n",
    "    def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "        r\"\"\"\n",
    "        Gán mỗi anchor với một ground truth box dựa trên IOU (Intersection Over Union).\n",
    "        Đồng thời tạo nhãn phân loại để sử dụng trong quá trình huấn luyện:\n",
    "        - label = 1: Với các anchor có IOU lớn hơn ngưỡng `high_iou_threshold`.\n",
    "        - label = 0: Với các anchor có IOU nhỏ hơn ngưỡng `low_iou_threshold`.\n",
    "        - label = -1: Với các anchor có IOU nằm trong khoảng giữa (`low_iou_threshold`, `high_iou_threshold`).\n",
    "\n",
    "        :param anchors: Tensor có kích thước (num_anchors_in_image, 4) chứa tất cả anchor boxes.\n",
    "        :param gt_boxes: Tensor có kích thước (num_gt_boxes_in_image, 4) chứa tất cả ground truth boxes.\n",
    "        :return:\n",
    "            - labels: Tensor (num_anchors_in_image) với giá trị {-1/0/1}.\n",
    "            - matched_gt_boxes: Tensor (num_anchors_in_image, 4), tọa độ của ground truth box gán cho từng anchor.\n",
    "                Ngay cả các anchor thuộc background hoặc bị bỏ qua cũng được gán với một ground truth box.\n",
    "                Điều này không gây vấn đề vì nhãn sẽ phân biệt chúng sau này.\n",
    "        \"\"\"\n",
    "        # Tính ma trận IOU giữa gt_boxes và anchors\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "\n",
    "        # Lấy IOU lớn nhất và chỉ số gt_box tương ứng cho mỗi anchor\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        \n",
    "        # Sao chép chỉ số ban đầu của gt_boxes để sử dụng sau này\n",
    "        best_match_gt_idx_pre_thresholding = best_match_gt_idx.clone()\n",
    "\n",
    "        # Cập nhật giá trị của best_match_gt_idx dựa trên ngưỡng IOU\n",
    "        below_low_threshold = best_match_iou < self.low_iou_threshold\n",
    "        between_thresholds = (best_match_iou >= self.low_iou_threshold) & (best_match_iou < self.high_iou_threshold)\n",
    "        best_match_gt_idx[below_low_threshold] = -1  # Background\n",
    "        best_match_gt_idx[between_thresholds] = -2  # Bị bỏ qua\n",
    "\n",
    "        # Đảm bảo các ground truth boxes có ít nhất một anchor chất lượng thấp được gán\n",
    "        best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        gt_pred_pair_with_highest_iou = torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "        pred_inds_to_update = gt_pred_pair_with_highest_iou[1]\n",
    "        best_match_gt_idx[pred_inds_to_update] = best_match_gt_idx_pre_thresholding[pred_inds_to_update]\n",
    "\n",
    "        # Chuyển đổi chỉ số gt_box để hợp lệ\n",
    "        matched_gt_boxes = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "\n",
    "        # Tạo nhãn phân loại\n",
    "        labels = torch.full_like(best_match_gt_idx, fill_value=-1, dtype=torch.float32)\n",
    "        labels[best_match_gt_idx >= 0] = 1.0  # Foreground\n",
    "        labels[best_match_gt_idx == -1] = 0.0  # Background\n",
    "\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        r\"\"\"\n",
    "        Hàm này thực hiện các bước lọc và điều chỉnh danh sách đề xuất (proposals) nhằm tối ưu hóa đầu vào \n",
    "        cho các bước tiếp theo trong mạng phát hiện đối tượng.\n",
    "\n",
    "        Các bước thực hiện bao gồm:\n",
    "        1. **Lọc top K proposals trước NMS** (Pre-NMS topK filtering): Chọn ra các proposals có điểm số cao nhất.\n",
    "        2. **Điều chỉnh tọa độ proposals hợp lệ**: Đảm bảo rằng tất cả các proposals nằm trong giới hạn hình ảnh.\n",
    "        3. **Loại bỏ các hộp nhỏ**: Loại bỏ những proposals có chiều rộng hoặc chiều cao nhỏ hơn kích thước tối thiểu.\n",
    "        4. **NMS (Non-Maximum Suppression)**: Loại bỏ các proposals trùng lặp dựa trên điểm số và ngưỡng IOU.\n",
    "        5. **Lọc top K proposals sau NMS** (Post-NMS topK filtering): Giữ lại số lượng proposals tốt nhất sau NMS.\n",
    "\n",
    "        :param proposals: Tensor (num_anchors_in_image, 4), tọa độ của các proposals (x_min, y_min, x_max, y_max).\n",
    "        :param cls_scores: Tensor (num_anchors_in_image), điểm số dự đoán cho từng proposal (logits).\n",
    "        :param image_shape: Tuple (height, width) của hình ảnh đã được resize, dùng để giới hạn tọa độ proposals.\n",
    "        :return: \n",
    "            - **proposals**: Tensor (num_filtered_proposals, 4), các proposals sau khi lọc.\n",
    "            - **cls_scores**: Tensor (num_filtered_proposals), điểm số tương ứng với các proposals đã lọc.\n",
    "        \"\"\"\n",
    "        # Bước 1: Lọc top K proposals trước NMS\n",
    "        cls_scores = cls_scores.reshape(-1)\n",
    "        cls_scores = torch.sigmoid(cls_scores)\n",
    "        _, top_n_idx = cls_scores.topk(min(self.rpn_prenms_topk, len(cls_scores)))\n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "        \n",
    "        # Bước 2: Giới hạn các proposals trong biên hình ảnh\n",
    "        proposals = clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "        \n",
    "        # Bước 3: Lọc bỏ các hộp nhỏ\n",
    "        min_size = 16  # Kích thước tối thiểu\n",
    "        ws, hs = proposals[:, 2] - proposals[:, 0], proposals[:, 3] - proposals[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        proposals = proposals[keep]\n",
    "        cls_scores = cls_scores[keep]\n",
    "        \n",
    "        # Bước 4: Áp dụng NMS (Non-Maximum Suppression)\n",
    "        keep_mask = torch.zeros_like(cls_scores, dtype=torch.bool)\n",
    "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, self.rpn_nms_threshold)\n",
    "        keep_mask[keep_indices] = True\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        \n",
    "        # Sắp xếp lại theo điểm số objectness (điểm số cao nhất trước)\n",
    "        post_nms_keep_indices = keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "        \n",
    "        # Bước 5: Lọc top K proposals sau NMS\n",
    "        proposals, cls_scores = (\n",
    "            proposals[post_nms_keep_indices[:self.rpn_topk]],\n",
    "            cls_scores[post_nms_keep_indices[:self.rpn_topk]],\n",
    "        )\n",
    "        \n",
    "        return proposals, cls_scores\n",
    "\n",
    "    \n",
    "    def forward(self, image, feat, target=None):\n",
    "        \"\"\"\n",
    "        Phương pháp chính cho RPN (Region Proposal Network) thực hiện các bước sau:\n",
    "        1. Gọi các lớp RPN cụ thể để tạo dự đoán phân loại (classification) và\n",
    "        biến đổi hộp giới hạn (bbox transformation) cho các anchors.\n",
    "        2. Sinh các anchors cho toàn bộ hình ảnh.\n",
    "        3. Biến đổi các anchors dựa trên dự đoán biến đổi hộp giới hạn để tạo ra các proposals.\n",
    "        4. Lọc các proposals.\n",
    "        5. Trong quá trình huấn luyện, thực hiện thêm các bước:\n",
    "            a. Gán nhãn mục tiêu và hộp giới hạn ground truth cho từng anchor.\n",
    "            b. Lấy mẫu các anchors dương (positive) và âm (negative).\n",
    "            c. Tính toán tổn thất phân loại sử dụng các anchors dương/âm được lấy mẫu.\n",
    "            d. Tính toán tổn thất định vị (localization loss) sử dụng các anchors dương.\n",
    "\n",
    "        Tham số:\n",
    "        - image: Tensor biểu diễn hình ảnh đầu vào.\n",
    "        - feat: Tensor đặc trưng đầu vào từ backbone.\n",
    "        - target: (Tùy chọn) Ground truth bao gồm nhãn và hộp giới hạn.\n",
    "\n",
    "        Trả về:\n",
    "        - rpn_output: Dictionary chứa các proposals, điểm số (scores), và mất mát (nếu có).\n",
    "        \"\"\"\n",
    "        # Bước 1: Tính toán các đặc trưng RPN\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "        # Bước 2: Sinh anchors\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "\n",
    "        # Định hình lại cls_scores thành (Batch_Size * H_feat * W_feat * Số anchors mỗi vị trí, 1)\n",
    "        number_of_anchors_per_location = cls_scores.size(1)\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1).reshape(-1, 1)\n",
    "\n",
    "        # Định hình lại box_transform_pred thành (Batch_Size * H_feat * W_feat * Số anchors mỗi vị trí, 4)\n",
    "        box_transform_pred = box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1]\n",
    "        ).permute(0, 3, 4, 1, 2).reshape(-1, 4)\n",
    "\n",
    "        # Bước 3: Biến đổi anchors thành proposals dựa trên box_transform_pred\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1, 1, 4),\n",
    "            anchors\n",
    "        )\n",
    "        proposals = proposals.reshape(proposals.size(0), 4)\n",
    "\n",
    "        # Bước 4: Lọc proposals\n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "        rpn_output = {'proposals': proposals, 'scores': scores}\n",
    "\n",
    "        if not self.training or target is None:\n",
    "            # Nếu không huấn luyện, trả về proposals và scores\n",
    "            return rpn_output\n",
    "\n",
    "        # Bước 5: Gán nhãn ground truth cho từng anchor\n",
    "        labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_targets_to_anchors(\n",
    "            anchors, target['bboxes'][0]\n",
    "        )\n",
    "\n",
    "        # Tính toán regression targets từ anchors và matched_gt_boxes\n",
    "        regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_anchors, anchors)\n",
    "\n",
    "        # Lấy mẫu anchors dương và âm\n",
    "        sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(\n",
    "            labels_for_anchors,\n",
    "            positive_count=self.rpn_pos_count,\n",
    "            total_count=self.rpn_batch_size\n",
    "        )\n",
    "\n",
    "        sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "\n",
    "        # Tính toán mất mát localization\n",
    "        localization_loss = (\n",
    "            torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[sampled_pos_idx_mask],\n",
    "                regression_targets[sampled_pos_idx_mask],\n",
    "                beta=1 / 9,\n",
    "                reduction=\"sum\",\n",
    "            ) / sampled_idxs.numel()\n",
    "        )\n",
    "\n",
    "        # Tính toán mất mát phân loại\n",
    "        cls_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            cls_scores[sampled_idxs].flatten(),\n",
    "            labels_for_anchors[sampled_idxs].flatten()\n",
    "        )\n",
    "\n",
    "        rpn_output['rpn_classification_loss'] = cls_loss\n",
    "        rpn_output['rpn_localization_loss'] = localization_loss\n",
    "        return rpn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adbd98",
   "metadata": {
    "papermill": {
     "duration": 0.006464,
     "end_time": "2024-12-25T16:20:52.451622",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.445158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. ROI Head\n",
    "\n",
    "Tiếp theo, em sẽ xây dựng một **RoI Head** với mục tiêu trích xuất các đặc trưng từ các vùng được đề xuất từ bước **RPN**, làm tiền đề cho phân loại và hồi quy cho bounding box.\n",
    "\n",
    "Sau khi có các **region proposals** từ **Region Proposal Network (RPN)**, RoI Head sử dụng các RoI này để trích xuất các đặc trưng từ đặc trưng toàn cục của ảnh. Các RoI có thể có kích thước khác nhau, nhưng RoI Head sử dụng kỹ thuật **RoI Pooling** để chuẩn hóa tất cả các RoI về kích thước cố định, giúp dễ dàng xử lý trong các bước tiếp theo.\n",
    "\n",
    "#### Các phương thức chính trong RoI Head:\n",
    "\n",
    "1. **assign_target_to_proposals**:\n",
    "   - Gán nhãn cho các đề xuất (**proposals**) dựa trên sự tương quan với các hộp **ground truth (gt_boxes)**. Mỗi proposal sẽ được phân loại là foreground, background hoặc ignored tùy theo sự tương quan với các hộp ground truth.\n",
    "\n",
    "2. **filter_predictions**:\n",
    "   - Lọc ra các proposal, nhưng lần này cần đảm bảo rằng các đề xuất được lọc chính xác để tạo thành đầu ra của mô hình, đảm bảo chất lượng của các dự đoán.\n",
    "\n",
    "3. **forward**:\n",
    "   Phương thức chính thực hiện các bước sau:\n",
    "   1. Nếu đang huấn luyện:\n",
    "      - Gán các box và nhãn mục tiêu cho tất cả các proposal.\n",
    "      - Lấy mẫu các proposal dương tính và tiêu cực.\n",
    "      - Lấy mục tiêu biến đổi bounding box cho tất cả các proposal dựa trên các nhãn được gán.\n",
    "   2. Lấy các đặc trưng đã qua **RoI Pooling** cho tất cả các proposal.\n",
    "   3. Gọi các lớp **fully connected (fc6, fc7)** và các lớp phân loại và hồi quy bounding box.\n",
    "   4. Tính toán lỗi phân loại và lỗi vị trí (localization loss).\n",
    "\n",
    "#### Các lớp và kỹ thuật trong RoI Head:\n",
    "\n",
    "- **RoI Pooling**: Kỹ thuật này giúp trích xuất các đặc trưng từ các RoI có kích thước khác nhau và chuẩn hóa chúng về kích thước cố định, từ đó giúp xử lý các RoI trong các bước tiếp theo của mô hình.\n",
    "\n",
    "- **Fully Connected Layers (fc6, fc7)**: Các lớp fully connected này giúp chuyển đổi các đặc trưng đã qua RoI Pooling thành các đặc trưng cuối cùng, phục vụ cho phân loại và dự đoán bounding box.\n",
    "\n",
    "- **Loss Calculation**:\n",
    "   - **Classification Loss**: Được tính bằng Cross-Entropy Loss giữa nhãn thực tế (ground truth) và nhãn dự đoán của các RoI.\n",
    "   - **Localization Loss**: Được tính bằng **Smooth L1 Loss** hoặc **L2 Loss** giữa các tọa độ thực tế (ground truth bbox) và các tọa độ dự đoán của các RoI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6015cb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.467430Z",
     "iopub.status.busy": "2024-12-25T16:20:52.467064Z",
     "iopub.status.idle": "2024-12-25T16:20:52.486981Z",
     "shell.execute_reply": "2024-12-25T16:20:52.486272Z"
    },
    "papermill": {
     "duration": 0.030094,
     "end_time": "2024-12-25T16:20:52.488230",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.458136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ROIHead(nn.Module):\n",
    "    r\"\"\"\n",
    "    ROI head trên lớp ROI pooling để tạo ra các dự đoán phân loại và biến đổi box.\n",
    "    Chúng ta có hai lớp fc (fully connected) theo sau là lớp fc phân loại và lớp fc hồi quy bbox.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, model_config, num_classes, in_channels):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.roi_batch_size = model_config['roi_batch_size']\n",
    "        self.roi_pos_count = int(model_config['roi_pos_fraction'] * self.roi_batch_size)\n",
    "        self.iou_threshold = model_config['roi_iou_threshold']\n",
    "        self.low_bg_iou = model_config['roi_low_bg_iou']\n",
    "        self.nms_threshold = model_config['roi_nms_threshold']\n",
    "        self.topK_detections = model_config['roi_topk_detections']\n",
    "        self.low_score_threshold = model_config['roi_score_threshold']\n",
    "        self.pool_size = model_config['roi_pool_size']\n",
    "        self.fc_inner_dim = model_config['fc_inner_dim']\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "        \n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "        torch.nn.init.constant_(self.cls_layer.bias, 0)\n",
    "\n",
    "        torch.nn.init.normal_(self.bbox_reg_layer.weight, std=0.001)\n",
    "        torch.nn.init.constant_(self.bbox_reg_layer.bias, 0)\n",
    "    \n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        r\"\"\"\n",
    "        Gán các đề xuất (proposals) cho các hộp ground truth (gt_boxes) và nhãn của chúng (gt_labels) \n",
    "        dựa trên giá trị IOU (Intersection over Union). \n",
    "        Sử dụng IOU để gán các đề xuất cho hộp ground truth hoặc background.\n",
    "        \n",
    "        :param proposals: (số lượng đề xuất, 4) - Các hộp đề xuất (proposals) trong không gian ảnh.\n",
    "        :param gt_boxes: (số lượng hộp ground truth, 4) - Các hộp ground truth (gt_boxes).\n",
    "        :param gt_labels: (số lượng hộp ground truth) - Các nhãn của các hộp ground truth.\n",
    "        \n",
    "        :return:\n",
    "            - labels: (số lượng đề xuất) - Nhãn được gán cho các đề xuất, bao gồm các nhãn của đối tượng hoặc background.\n",
    "            - matched_gt_boxes: (số lượng đề xuất, 4) - Các hộp ground truth được gán cho các đề xuất.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tính toán ma trận IOU giữa các hộp ground truth và các đề xuất\n",
    "        iou_matrix = get_iou(gt_boxes, proposals)\n",
    "        \n",
    "        # Tìm hộp ground truth phù hợp nhất cho từng đề xuất\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        \n",
    "        # Đánh dấu các đề xuất thuộc về background (IOU thấp hơn threshold)\n",
    "        background_proposals = (best_match_iou < self.iou_threshold) & (best_match_iou >= self.low_bg_iou)\n",
    "        \n",
    "        # Đánh dấu các đề xuất bị bỏ qua (IOU quá thấp)\n",
    "        ignored_proposals = best_match_iou < self.low_bg_iou\n",
    "        \n",
    "        # Cập nhật chỉ mục của các đề xuất thuộc background và bị bỏ qua\n",
    "        best_match_gt_idx[background_proposals] = -1\n",
    "        best_match_gt_idx[ignored_proposals] = -2\n",
    "        \n",
    "        # Lấy các hộp ground truth phù hợp nhất cho tất cả các đề xuất\n",
    "        # Các đề xuất background cũng sẽ có một hộp ground truth được gán\n",
    "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "        \n",
    "        # Lấy nhãn của các đề xuất theo các hộp ground truth phù hợp\n",
    "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "        \n",
    "        # Gán nhãn 0 (background) cho các đề xuất thuộc background\n",
    "        labels[background_proposals] = 0\n",
    "        \n",
    "        # Gán nhãn -1 cho các đề xuất bị bỏ qua (ignored proposals)\n",
    "        labels[ignored_proposals] = -1\n",
    "        \n",
    "        return labels, matched_gt_boxes_for_proposals\n",
    "        \n",
    "    def forward(self, feat, proposals, image_shape, target):\n",
    "        r\"\"\"\n",
    "        Phương thức chính cho ROI head thực hiện các bước sau:\n",
    "        1. Nếu đang huấn luyện, gán các box và nhãn mục tiêu cho tất cả các proposal.\n",
    "        2. Nếu đang huấn luyện, lấy mẫu các proposal dương tính và tiêu cực.\n",
    "        3. Nếu đang huấn luyện, lấy mục tiêu biến đổi bbox cho tất cả các proposal dựa trên các gán nhãn.\n",
    "        4. Lấy các đặc trưng đã qua ROI Pooling cho tất cả các proposal.\n",
    "        5. Gọi các lớp fc6, fc7 và các lớp phân loại và biến đổi bbox.\n",
    "        6. Tính toán lỗi phân loại và lỗi vị trí (localization).\n",
    "\n",
    "        :param feat: Tensor đặc trưng của hình ảnh đầu vào.\n",
    "        :param proposals: Các proposal dự đoán, có dạng (số lượng proposal, 4).\n",
    "        :param image_shape: Kích thước của hình ảnh đầu vào (height, width).\n",
    "        :param target: Mục tiêu trong quá trình huấn luyện, chứa các hộp gốc và nhãn.\n",
    "        :return: frcnn_output: Một dictionary chứa các thông tin đầu ra của mô hình, bao gồm:\n",
    "                - Hệ số phân loại và vị trí trong quá trình huấn luyện.\n",
    "                - Các hộp, nhãn và điểm số trong quá trình suy luận.\n",
    "        \"\"\"\n",
    "        if self.training and target is not None:\n",
    "            # Thêm ground truth vào proposals\n",
    "            proposals = torch.cat([proposals, target['bboxes'][0]], dim=0)\n",
    "            \n",
    "            gt_boxes = target['bboxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "            \n",
    "            # Gán mục tiêu cho proposals\n",
    "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(proposals, gt_boxes, gt_labels)\n",
    "            \n",
    "            # Lấy mẫu các proposal dương tính và tiêu cực\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negative(labels,\n",
    "                                                                                positive_count=self.roi_pos_count,\n",
    "                                                                                total_count=self.roi_batch_size)\n",
    "            \n",
    "            # Chọn các proposal đã được lấy mẫu\n",
    "            sampled_idxs = torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "            \n",
    "            # Chỉ giữ lại các proposal đã được lấy mẫu\n",
    "            proposals = proposals[sampled_idxs]\n",
    "            labels = labels[sampled_idxs]\n",
    "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
    "            \n",
    "            # Lấy các mục tiêu biến đổi bbox\n",
    "            regression_targets = boxes_to_transformation_targets(matched_gt_boxes_for_proposals, proposals)\n",
    "        \n",
    "        # Tính toán tỷ lệ (scale) cần thiết cho ROI Pooling\n",
    "        size = feat.shape[-2:]\n",
    "        possible_scales = []\n",
    "        for s1, s2 in zip(size, image_shape):\n",
    "            approx_scale = float(s1) / float(s2)\n",
    "            scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
    "            possible_scales.append(scale)\n",
    "        assert possible_scales[0] == possible_scales[1]\n",
    "        \n",
    "        # ROI Pooling và gọi các lớp để dự đoán\n",
    "        proposal_roi_pool_feats = torchvision.ops.roi_pool(feat, [proposals],\n",
    "                                                        output_size=self.pool_size,\n",
    "                                                        spatial_scale=possible_scales[0])\n",
    "        proposal_roi_pool_feats = proposal_roi_pool_feats.flatten(start_dim=1)\n",
    "        \n",
    "        # Dự đoán thông qua các lớp fc6 và fc7\n",
    "        box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_pool_feats))\n",
    "        box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
    "        \n",
    "        # Tính toán các dự đoán phân loại và biến đổi bbox\n",
    "        cls_scores = self.cls_layer(box_fc_7)\n",
    "        box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
    "        \n",
    "        num_boxes, num_classes = cls_scores.shape\n",
    "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "        \n",
    "        frcnn_output = {}\n",
    "        \n",
    "        if self.training and target is not None:\n",
    "            # Tính toán lỗi phân loại\n",
    "            classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
    "            \n",
    "            # Tính toán lỗi vị trí chỉ cho các proposals không phải background\n",
    "            fg_proposals_idxs = torch.where(labels > 0)[0]\n",
    "            fg_cls_labels = labels[fg_proposals_idxs]\n",
    "            \n",
    "            # Lỗi vị trí (localization loss) cho các proposal dương tính\n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[fg_proposals_idxs, fg_cls_labels],\n",
    "                regression_targets[fg_proposals_idxs],\n",
    "                beta=1/9,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            localization_loss = localization_loss / labels.numel()\n",
    "            \n",
    "            # Lưu kết quả lỗi phân loại và vị trí\n",
    "            frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "            frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "        \n",
    "        if self.training:\n",
    "            return frcnn_output\n",
    "        else:\n",
    "            device = cls_scores.device\n",
    "            \n",
    "            # Áp dụng các dự đoán biến đổi bbox lên các proposals\n",
    "            pred_boxes = apply_regression_pred_to_anchors_or_proposals(box_transform_pred, proposals)\n",
    "            \n",
    "            # Áp dụng softmax cho điểm số phân loại\n",
    "            pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
    "            \n",
    "            # Clamp hộp dự đoán vào giới hạn của hình ảnh\n",
    "            pred_boxes = clamp_boxes_to_image_boundary(pred_boxes, image_shape)\n",
    "            \n",
    "            # Tạo nhãn cho mỗi dự đoán\n",
    "            pred_labels = torch.arange(num_classes, device=device)\n",
    "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "            \n",
    "            # Loại bỏ các dự đoán có nhãn background\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "            \n",
    "            # Batch các dự đoán cho mỗi lớp\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            \n",
    "            # Lọc các dự đoán sau khi NMS\n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(pred_boxes, pred_labels, pred_scores)\n",
    "            \n",
    "            # Lưu các kết quả dự đoán\n",
    "            frcnn_output['boxes'] = pred_boxes\n",
    "            frcnn_output['scores'] = pred_scores\n",
    "            frcnn_output['labels'] = pred_labels\n",
    "            return frcnn_output\n",
    "\n",
    "    \n",
    "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
    "        r\"\"\"\n",
    "        Phương thức để lọc các dự đoán (predictions) bằng cách thực hiện các bước sau:\n",
    "        1. Lọc các hộp có điểm số thấp.\n",
    "        2. Loại bỏ các hộp có kích thước nhỏ.\n",
    "        3. Thực hiện NMS (Non-Maximum Suppression) cho từng lớp riêng biệt.\n",
    "        4. Giữ lại chỉ topK phát hiện.\n",
    "\n",
    "        :param pred_boxes: (tensor) Các hộp dự đoán, kích thước (num_boxes, 4), mỗi hộp có định dạng [x1, y1, x2, y2].\n",
    "        :param pred_labels: (tensor) Các nhãn dự đoán, kích thước (num_boxes), chứa các lớp cho mỗi hộp.\n",
    "        :param pred_scores: (tensor) Các điểm số dự đoán, kích thước (num_boxes), chứa độ tin cậy của mỗi hộp.\n",
    "        :return: \n",
    "            - pred_boxes: (tensor) Các hộp dự đoán sau khi lọc, kích thước (num_filtered_boxes, 4).\n",
    "            - pred_labels: (tensor) Các nhãn dự đoán tương ứng sau khi lọc.\n",
    "            - pred_scores: (tensor) Các điểm số dự đoán sau khi lọc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Lọc các hộp có điểm số thấp\n",
    "        keep = torch.where(pred_scores > self.low_score_threshold)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Loại bỏ các hộp có kích thước nhỏ\n",
    "        min_size = 16\n",
    "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        # Thực hiện NMS cho từng lớp riêng biệt\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torch.ops.torchvision.nms(pred_boxes[curr_indices],\n",
    "                                                        pred_scores[curr_indices],\n",
    "                                                        self.nms_threshold)\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        \n",
    "        # Giữ lại các hộp dự đoán sau khi NMS\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "        keep = post_nms_keep_indices[:self.topK_detections]\n",
    "        \n",
    "        # Lọc các hộp dự đoán theo chỉ số đã chọn\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "        \n",
    "        return pred_boxes, pred_labels, pred_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf70e2",
   "metadata": {
    "papermill": {
     "duration": 0.006766,
     "end_time": "2024-12-25T16:20:52.501783",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.495017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Faster RCNN\n",
    "Cuối cùng xây dựng một faster RCNN hoàn chỉnh bao gồm RPN và RoI head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e0d38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.517308Z",
     "iopub.status.busy": "2024-12-25T16:20:52.516957Z",
     "iopub.status.idle": "2024-12-25T16:20:52.522147Z",
     "shell.execute_reply": "2024-12-25T16:20:52.521446Z"
    },
    "papermill": {
     "duration": 0.014334,
     "end_time": "2024-12-25T16:20:52.523390",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.509056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
    "    \"\"\"\n",
    "    Hàm chuyển đổi các hộp chứa (bounding boxes) từ kích thước đã thay đổi (resize) \n",
    "    về kích thước gốc của ảnh trước khi thay đổi kích thước.\n",
    "\n",
    "    Khi ảnh được thay đổi kích thước, các hộp chứa cần được điều chỉnh lại \n",
    "    để phù hợp với kích thước gốc của ảnh. Hàm này thực hiện phép toán tỷ lệ \n",
    "    giữa kích thước ảnh đã thay đổi và kích thước gốc, sau đó áp dụng tỷ lệ này \n",
    "    để điều chỉnh các tọa độ của hộp chứa.\n",
    "\n",
    "    Args:\n",
    "        boxes (Tensor): Tensor chứa các hộp chứa sau khi thay đổi kích thước, có dạng (Batchsize x N x 4), \n",
    "                         trong đó mỗi hộp chứa có 4 tọa độ [xmin, ymin, xmax, ymax].\n",
    "        new_size (tuple): Kích thước của ảnh sau khi thay đổi kích thước (new_height, new_width).\n",
    "        original_size (tuple): Kích thước gốc của ảnh trước khi thay đổi kích thước (original_height, original_width).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Tensor chứa các hộp chứa sau khi được chuyển đổi về kích thước gốc của ảnh, có cùng dạng với `boxes`.\n",
    "    \"\"\"\n",
    "    # Tính toán tỷ lệ giữa kích thước gốc và kích thước đã thay đổi\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "\n",
    "    # Tách các tọa độ xmin, ymin, xmax, ymax của các hộp chứa\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "\n",
    "    # Điều chỉnh các tọa độ của hộp chứa theo tỷ lệ đã tính toán\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "\n",
    "    # Trả lại các hộp chứa đã được điều chỉnh về kích thước gốc\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bce7434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.538071Z",
     "iopub.status.busy": "2024-12-25T16:20:52.537819Z",
     "iopub.status.idle": "2024-12-25T16:20:52.550571Z",
     "shell.execute_reply": "2024-12-25T16:20:52.549740Z"
    },
    "papermill": {
     "duration": 0.022117,
     "end_time": "2024-12-25T16:20:52.552125",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.530008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, model_config, num_classes, backbone):\n",
    "        \"\"\"\n",
    "        Hàm khởi tạo cho lớp FasterRCNN.\n",
    "\n",
    "        Args:\n",
    "            model_config (dict): Cấu hình của mô hình, chứa các thông số như kích thước đầu vào của ảnh, số kênh đầu ra của backbone, các tỉ lệ tỉ lệ của RPN, ...\n",
    "            num_classes (int): Số lượng lớp cần phân loại (bao gồm cả lớp nền).\n",
    "            backbone (nn.Module): Mạng trích xuất đặc trưng (ví dụ: VGG16, ResNet, v.v.)\n",
    "                - input: Batchsize x in_channels x H x W\n",
    "                - output: Batchsize x out_channels x H_out x W_out (ví dụ: 20 -> 40)\n",
    "        \"\"\"\n",
    "        super(FasterRCNN, self).__init__()\n",
    "\n",
    "        # Lưu lại cấu hình của mô hình\n",
    "        self.model_config = model_config\n",
    "        \n",
    "        # Backbone sẽ dùng để trích xuất đặc trưng\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Khởi tạo Region Proposal Network (RPN) với các tham số từ cấu hình\n",
    "        self.rpn = RegionProposalNetwork(\n",
    "            model_config['backbone_out_channels'],\n",
    "            scales=model_config['scales'],\n",
    "            aspect_ratios=model_config['aspect_ratios'],\n",
    "            model_config=model_config\n",
    "        )\n",
    "\n",
    "        # Khởi tạo ROIHead để xử lý các đề xuất (proposals)\n",
    "        self.roi_head = ROIHead(model_config, num_classes, in_channels=model_config['backbone_out_channels'])\n",
    "\n",
    "        # Khởi tạo các thông số cho ảnh\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        # Kích thước tối thiểu và tối đa của ảnh trong quá trình huấn luyện\n",
    "        self.min_size = model_config['min_im_size']\n",
    "        self.max_size = model_config['max_im_size']\n",
    "\n",
    "    \n",
    "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
    "        \"\"\"\n",
    "        Hàm chuẩn hóa và thay đổi kích thước ảnh và các hộp chứa (bounding boxes).\n",
    "\n",
    "        Các bước thực hiện:\n",
    "        1. Chuẩn hóa ảnh bằng cách trừ đi giá trị trung bình và chia cho độ lệch chuẩn.\n",
    "        2. Thay đổi kích thước ảnh sao cho chiều nhỏ nhất được thay đổi tới 600, và chiều lớn hơn không vượt quá 1000.\n",
    "        3. Nếu có, thay đổi kích thước của các hộp chứa tương ứng với kích thước ảnh mới.\n",
    "\n",
    "        Args:\n",
    "            image (Tensor): Tensor ảnh đầu vào có dạng (Batchsize x Channels x Height x Width).\n",
    "            bboxes (Tensor, optional): Tensor chứa các hộp chứa với dạng (Batchsize x N_boxes x 4). \n",
    "                                    Mỗi hộp chứa được biểu diễn dưới dạng [xmin, ymin, xmax, ymax].\n",
    "\n",
    "        Returns:\n",
    "            image (Tensor): Ảnh sau khi chuẩn hóa và thay đổi kích thước.\n",
    "            bboxes (Tensor, optional): Các hộp chứa sau khi thay đổi kích thước, nếu có.\n",
    "        \"\"\"\n",
    "        dtype, device = image.dtype, image.device\n",
    "        \n",
    "        # Chuẩn hóa ảnh\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=dtype, device=device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=dtype, device=device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "        # Thay đổi kích thước ảnh sao cho chiều nhỏ nhất được thay đổi tới 600, và chiều lớn hơn không vượt quá 1000\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(dtype=torch.float32)\n",
    "        max_size = torch.max(im_shape).to(dtype=torch.float32)\n",
    "        \n",
    "        # Tính tỷ lệ thay đổi kích thước sao cho chiều nhỏ nhất là 600 và chiều lớn nhất không vượt quá 1000\n",
    "        scale = torch.min(float(self.min_size) / min_size, float(self.max_size) / max_size)\n",
    "        scale_factor = scale.item()\n",
    "\n",
    "        # Thay đổi kích thước ảnh dựa trên tỷ lệ đã tính toán\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            size=None,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=\"bilinear\",\n",
    "            recompute_scale_factor=True,\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            # Thay đổi kích thước các hộp chứa (bboxes) tương ứng với tỷ lệ thay đổi kích thước của ảnh\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            \n",
    "            # Cập nhật vị trí của các hộp chứa\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n",
    "\n",
    "        return image, bboxes\n",
    "\n",
    "    \n",
    "    def forward(self, image, target=None):\n",
    "        \"\"\"\n",
    "        Hàm thực hiện bước truyền qua (forward pass) trong mô hình Faster R-CNN.\n",
    "\n",
    "        Các bước thực hiện:\n",
    "        1. Chuẩn hóa và thay đổi kích thước ảnh và các hộp chứa (bounding boxes) trong trường hợp huấn luyện.\n",
    "        2. Tiến hành trích xuất đặc trưng từ ảnh thông qua mạng xương sống (backbone).\n",
    "        3. Sử dụng Mạng Đề xuất Vùng (RPN) để sinh ra các đề xuất (proposals).\n",
    "        4. Sử dụng ROI head để biến các đề xuất thành các hộp chứa (bounding boxes) cuối cùng.\n",
    "        5. Nếu không phải trong quá trình huấn luyện, trả lại các hộp chứa đã được chuyển đổi về kích thước ảnh gốc.\n",
    "\n",
    "        Args:\n",
    "            image (Tensor): Tensor ảnh đầu vào có dạng (Batchsize x Channels x Height x Width).\n",
    "            target (dict, optional): Một từ điển chứa thông tin về các hộp chứa (bboxes) cho mỗi đối tượng trong ảnh, \n",
    "                                    được sử dụng trong quá trình huấn luyện. Từ điển có thể chứa:\n",
    "                                    - 'bboxes': các hộp chứa trong ảnh có dạng [xmin, ymin, xmax, ymax].\n",
    "                                    \n",
    "        Returns:\n",
    "            rpn_output (dict): Kết quả từ Mạng Đề xuất Vùng (RPN), bao gồm các đề xuất (proposals).\n",
    "            frcnn_output (dict): Kết quả từ ROI head, bao gồm các hộp chứa cuối cùng và các thông tin liên quan.\n",
    "        \"\"\"\n",
    "        old_shape = image.shape[-2:]\n",
    "\n",
    "        if self.training:\n",
    "            # Chuẩn hóa và thay đổi kích thước ảnh và các hộp chứa trong trường hợp huấn luyện\n",
    "            image, bboxes = self.normalize_resize_image_and_boxes(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            # Chỉ chuẩn hóa và thay đổi kích thước ảnh mà không có hộp chứa trong trường hợp suy luận (inference)\n",
    "            image, _ = self.normalize_resize_image_and_boxes(image, None)\n",
    "        \n",
    "        # Trích xuất đặc trưng từ ảnh thông qua backbone\n",
    "        feat = self.backbone(image)\n",
    "        \n",
    "        # Tiến hành sinh các đề xuất (proposals) từ RPN\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "        \n",
    "        # Biến các đề xuất thành các hộp chứa cuối cùng thông qua ROI head\n",
    "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
    "        \n",
    "        if not self.training:\n",
    "            # Chuyển đổi các hộp chứa về kích thước gốc của ảnh, chỉ được gọi trong quá trình suy luận\n",
    "            frcnn_output['boxes'] = transform_boxes_to_original_size(frcnn_output['boxes'],\n",
    "                                                                    image.shape[-2:],\n",
    "                                                                    old_shape)\n",
    "        \n",
    "        return rpn_output, frcnn_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ce5b9",
   "metadata": {
    "papermill": {
     "duration": 0.00757,
     "end_time": "2024-12-25T16:20:52.567588",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.560018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Xây dựng Dataset, Dataloader\n",
    "Sau khi hoàn thành việc thiết lập model, tiếp theo em sẽ thực hiện việc lấy dữ liệu\n",
    "đóng gói dữ liệu thành Dataset, Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7faf4765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:52.581900Z",
     "iopub.status.busy": "2024-12-25T16:20:52.581612Z",
     "iopub.status.idle": "2024-12-25T16:20:53.103817Z",
     "shell.execute_reply": "2024-12-25T16:20:53.102640Z"
    },
    "papermill": {
     "duration": 0.531777,
     "end_time": "2024-12-25T16:20:53.105947",
     "exception": false,
     "start_time": "2024-12-25T16:20:52.574170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "\n",
    "def parse_txt_to_dict(file_path):\n",
    "    result = {'labels': [], 'bboxes': []}\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Tách các giá trị trong dòng\n",
    "            values = line.split()\n",
    "            label = int(values[0])  # Nhãn\n",
    "            bbox = list(map(float, values[1:]))  # Hộp giới hạn (bounding box)\n",
    "            \n",
    "            # Thêm vào kết quả\n",
    "            result['labels'].append(label)\n",
    "            result['bboxes'].append(bbox)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_valid_file_in_folder(folder_path, valid_extensions):\n",
    "    valid_files_path = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_extension = os.path.splitext(file_name)[1].lower()  # Lấy phần mở rộng\n",
    "        if file_extension in valid_extensions:\n",
    "            valid_files_path.append(os.path.join(folder_path, file_name))\n",
    "\n",
    "    return valid_files_path\n",
    "\n",
    "def load_images_and_labels(folder_path):\n",
    "\n",
    "    # Lấy các file images\n",
    "    valid_extensions_image = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"}\n",
    "    folder_contain_images_path = os.path.join(folder_path, 'images')\n",
    "    image_paths = get_valid_file_in_folder(folder_contain_images_path, valid_extensions_image)\n",
    "\n",
    "    # Lấy các file labels\n",
    "    folder_contain_labels_path = os.path.join(folder_path, 'labels')\n",
    "    label_paths = []\n",
    "    for file_name in image_paths:\n",
    "        label_file_name = os.path.splitext(os.path.basename(file_name))[0] + '.txt'\n",
    "        label_paths.append(os.path.join(folder_contain_labels_path, label_file_name))\n",
    "    \n",
    "    return list(zip(image_paths, label_paths))\n",
    "    \n",
    "\n",
    "\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, pair_path_image_label):\n",
    "        # self.data = load_images_and_labels(folder_path)\n",
    "        self.data = pair_path_image_label\n",
    "        classes = [\n",
    "            'Apple', 'Grapes', 'Pineapple', 'Orange', 'Banana', 'Watermelon'\n",
    "        ]\n",
    "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, target_path = self.data[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        target = parse_txt_to_dict(target_path)\n",
    "\n",
    "        image_tensor = torchvision.transforms.ToTensor()(image)\n",
    "        target['labels'] = torch.as_tensor(target['labels'], dtype=torch.int64)\n",
    "        target['bboxes'] = torch.as_tensor(target['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image_tensor, target, torch.as_tensor(np.array(image))\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "test_size = 20\n",
    "folder_path = '/kaggle/input/k-67-object-detection/Dataset/Train'\n",
    "pair_path_image_label = load_images_and_labels(folder_path)\n",
    "\n",
    "random.shuffle(pair_path_image_label)\n",
    "\n",
    "train_pair_path_image_label = pair_path_image_label[:-test_size]\n",
    "test_pair_path_image_label = pair_path_image_label[-test_size:]\n",
    "\n",
    "# dataset = ObjectDetectionDataset(folder_path)\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(train_pair_path_image_label)\n",
    "test_dataset = ObjectDetectionDataset(test_pair_path_image_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f1fdd",
   "metadata": {
    "papermill": {
     "duration": 0.008457,
     "end_time": "2024-12-25T16:20:53.123649",
     "exception": false,
     "start_time": "2024-12-25T16:20:53.115192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4055ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:53.141657Z",
     "iopub.status.busy": "2024-12-25T16:20:53.141341Z",
     "iopub.status.idle": "2024-12-25T16:20:53.146925Z",
     "shell.execute_reply": "2024-12-25T16:20:53.146263Z"
    },
    "papermill": {
     "duration": 0.0168,
     "end_time": "2024-12-25T16:20:53.148168",
     "exception": false,
     "start_time": "2024-12-25T16:20:53.131368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_params\": {\n",
    "        \"im_channels\": 3,\n",
    "        \"aspect_ratios\": [0.5, 1, 2],\n",
    "        \"scales\": [128, 256, 512],\n",
    "        \"min_im_size\": 600,\n",
    "        \"max_im_size\": 1000,\n",
    "        \"backbone_out_channels\": 512,\n",
    "        \"fc_inner_dim\": 1024,\n",
    "        \"rpn_bg_threshold\": 0.3,\n",
    "        \"rpn_fg_threshold\": 0.7,\n",
    "        \"rpn_nms_threshold\": 0.7,\n",
    "        \"rpn_train_prenms_topk\": 12000,\n",
    "        \"rpn_test_prenms_topk\": 6000,\n",
    "        \"rpn_train_topk\": 2000,\n",
    "        \"rpn_test_topk\": 300,\n",
    "        \"rpn_batch_size\": 256,\n",
    "        \"rpn_pos_fraction\": 0.5,\n",
    "        \"roi_iou_threshold\": 0.5,\n",
    "        \"roi_low_bg_iou\": 0.0,  \n",
    "        \"roi_pool_size\": 7,\n",
    "        \"roi_nms_threshold\": 0.3,\n",
    "        \"roi_topk_detections\": 100,\n",
    "        \"roi_score_threshold\": 0.05,\n",
    "        \"roi_batch_size\": 128,\n",
    "        \"roi_pos_fraction\": 0.25,\n",
    "    },\n",
    "    \"train_params\": {\n",
    "        \"task_name\": \"fruit_detection\",\n",
    "        \"seed\": 1111,\n",
    "        \"acc_steps\": 1,\n",
    "        \"num_epochs\": 40, # 20\n",
    "        \"lr_steps\": [12, 16],\n",
    "        \"lr\": 0.001,\n",
    "        \"ckpt_name\": \"faster_rcnn_fruit_detection.pth\",\n",
    "        \"name_backbone\": \"default\"\n",
    "    },\n",
    "    \"dataset_params\": {\n",
    "        'num_classes' : 6\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_config = config['dataset_params']\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226c89c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:53.162511Z",
     "iopub.status.busy": "2024-12-25T16:20:53.162233Z",
     "iopub.status.idle": "2024-12-25T16:20:53.218805Z",
     "shell.execute_reply": "2024-12-25T16:20:53.218005Z"
    },
    "papermill": {
     "duration": 0.065001,
     "end_time": "2024-12-25T16:20:53.220013",
     "exception": false,
     "start_time": "2024-12-25T16:20:53.155012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def train(model_config, train_config, dataset_config, backbone):\n",
    "    \n",
    "    # dataset_config = config['dataset_params']\n",
    "    # model_config = config['model_params']\n",
    "    # train_config = config['train_params']\n",
    "    \n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, # train_dataset\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    faster_rcnn_model = FasterRCNN(\n",
    "        model_config, \n",
    "        num_classes=dataset_config['num_classes'],\n",
    "        backbone=backbone\n",
    "    )\n",
    "    faster_rcnn_model.train()\n",
    "    faster_rcnn_model.to(device)\n",
    "\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "\n",
    "    optimizer = torch.optim.SGD(lr=train_config['lr'],\n",
    "                                params=filter(lambda p: p.requires_grad,\n",
    "                                              faster_rcnn_model.parameters()),\n",
    "                                weight_decay=5E-4,\n",
    "                                momentum=0.9)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.1)\n",
    "    \n",
    "    acc_steps = train_config['acc_steps']\n",
    "    num_epochs = train_config['num_epochs']\n",
    "    step_count = 1\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        rpn_classification_losses = []\n",
    "        rpn_localization_losses = []\n",
    "        frcnn_classification_losses = []\n",
    "        frcnn_localization_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for im, target, _ in tqdm(train_dataloader):\n",
    "            im = im.float().to(device)\n",
    "            target['bboxes'] = target['bboxes'].float().to(device)\n",
    "            target['labels'] = target['labels'].long().to(device)\n",
    "            rpn_output, frcnn_output = faster_rcnn_model(im, target)\n",
    "            \n",
    "            rpn_loss = rpn_output['rpn_classification_loss'] + rpn_output['rpn_localization_loss']\n",
    "            frcnn_loss = frcnn_output['frcnn_classification_loss'] + frcnn_output['frcnn_localization_loss']\n",
    "            loss = rpn_loss + frcnn_loss\n",
    "            \n",
    "            rpn_classification_losses.append(rpn_output['rpn_classification_loss'].item())\n",
    "            rpn_localization_losses.append(rpn_output['rpn_localization_loss'].item())\n",
    "            frcnn_classification_losses.append(frcnn_output['frcnn_classification_loss'].item())\n",
    "            frcnn_localization_losses.append(frcnn_output['frcnn_localization_loss'].item())\n",
    "            loss = loss / acc_steps\n",
    "            loss.backward()\n",
    "            if step_count % acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            step_count += 1\n",
    "        print('Finished epoch {}'.format(i))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        path_save = os.path.join(train_config['task_name'], train_config['name_backbone'], train_config['ckpt_name'])\n",
    "        dir_save = os.path.dirname(path_save)\n",
    "        if not os.path.exists(dir_save):\n",
    "            os.makedirs(dir_save, exist_ok=True)\n",
    "        \n",
    "        torch.save(faster_rcnn_model.state_dict(), path_save)\n",
    "        loss_output = ''\n",
    "        loss_output += 'RPN Classification Loss : {:.4f}'.format(np.mean(rpn_classification_losses))\n",
    "        loss_output += ' | RPN Localization Loss : {:.4f}'.format(np.mean(rpn_localization_losses))\n",
    "        loss_output += ' | FRCNN Classification Loss : {:.4f}'.format(np.mean(frcnn_classification_losses))\n",
    "        loss_output += ' | FRCNN Localization Loss : {:.4f}'.format(np.mean(frcnn_localization_losses))\n",
    "        print(loss_output)\n",
    "        scheduler.step()\n",
    "    print('Train Completed!')\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44796584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:20:53.234578Z",
     "iopub.status.busy": "2024-12-25T16:20:53.234316Z",
     "iopub.status.idle": "2024-12-25T19:19:57.184490Z",
     "shell.execute_reply": "2024-12-25T19:19:57.183354Z"
    },
    "papermill": {
     "duration": 10743.958764,
     "end_time": "2024-12-25T19:19:57.185947",
     "exception": false,
     "start_time": "2024-12-25T16:20:53.227183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 202MB/s]\n",
      "100%|██████████| 7611/7611 [04:33<00:00, 27.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "RPN Classification Loss : 0.1712 | RPN Localization Loss : 0.0789 | FRCNN Classification Loss : 0.3376 | FRCNN Localization Loss : 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1\n",
      "RPN Classification Loss : 0.1103 | RPN Localization Loss : 0.0649 | FRCNN Classification Loss : 0.2152 | FRCNN Localization Loss : 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2\n",
      "RPN Classification Loss : 0.0800 | RPN Localization Loss : 0.0572 | FRCNN Classification Loss : 0.1585 | FRCNN Localization Loss : 0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3\n",
      "RPN Classification Loss : 0.0579 | RPN Localization Loss : 0.0517 | FRCNN Classification Loss : 0.1217 | FRCNN Localization Loss : 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 4\n",
      "RPN Classification Loss : 0.0434 | RPN Localization Loss : 0.0470 | FRCNN Classification Loss : 0.1009 | FRCNN Localization Loss : 0.0342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:28<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 5\n",
      "RPN Classification Loss : 0.0340 | RPN Localization Loss : 0.0430 | FRCNN Classification Loss : 0.0865 | FRCNN Localization Loss : 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 6\n",
      "RPN Classification Loss : 0.0276 | RPN Localization Loss : 0.0400 | FRCNN Classification Loss : 0.0762 | FRCNN Localization Loss : 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 7\n",
      "RPN Classification Loss : 0.0228 | RPN Localization Loss : 0.0370 | FRCNN Classification Loss : 0.0711 | FRCNN Localization Loss : 0.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 8\n",
      "RPN Classification Loss : 0.0201 | RPN Localization Loss : 0.0348 | FRCNN Classification Loss : 0.0670 | FRCNN Localization Loss : 0.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 9\n",
      "RPN Classification Loss : 0.0179 | RPN Localization Loss : 0.0333 | FRCNN Classification Loss : 0.0643 | FRCNN Localization Loss : 0.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 10\n",
      "RPN Classification Loss : 0.0163 | RPN Localization Loss : 0.0312 | FRCNN Classification Loss : 0.0618 | FRCNN Localization Loss : 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 11\n",
      "RPN Classification Loss : 0.0153 | RPN Localization Loss : 0.0303 | FRCNN Classification Loss : 0.0596 | FRCNN Localization Loss : 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 12\n",
      "RPN Classification Loss : 0.0114 | RPN Localization Loss : 0.0245 | FRCNN Classification Loss : 0.0514 | FRCNN Localization Loss : 0.0189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 13\n",
      "RPN Classification Loss : 0.0094 | RPN Localization Loss : 0.0220 | FRCNN Classification Loss : 0.0472 | FRCNN Localization Loss : 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 14\n",
      "RPN Classification Loss : 0.0085 | RPN Localization Loss : 0.0208 | FRCNN Classification Loss : 0.0454 | FRCNN Localization Loss : 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:28<00:00, 28.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 15\n",
      "RPN Classification Loss : 0.0078 | RPN Localization Loss : 0.0199 | FRCNN Classification Loss : 0.0441 | FRCNN Localization Loss : 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:30<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 16\n",
      "RPN Classification Loss : 0.0076 | RPN Localization Loss : 0.0192 | FRCNN Classification Loss : 0.0425 | FRCNN Localization Loss : 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:29<00:00, 28.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 17\n",
      "RPN Classification Loss : 0.0075 | RPN Localization Loss : 0.0191 | FRCNN Classification Loss : 0.0421 | FRCNN Localization Loss : 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 18\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0190 | FRCNN Classification Loss : 0.0419 | FRCNN Localization Loss : 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 19\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0190 | FRCNN Classification Loss : 0.0413 | FRCNN Localization Loss : 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 20\n",
      "RPN Classification Loss : 0.0073 | RPN Localization Loss : 0.0189 | FRCNN Classification Loss : 0.0413 | FRCNN Localization Loss : 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 21\n",
      "RPN Classification Loss : 0.0073 | RPN Localization Loss : 0.0189 | FRCNN Classification Loss : 0.0415 | FRCNN Localization Loss : 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:30<00:00, 28.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 22\n",
      "RPN Classification Loss : 0.0073 | RPN Localization Loss : 0.0188 | FRCNN Classification Loss : 0.0414 | FRCNN Localization Loss : 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:33<00:00, 27.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 23\n",
      "RPN Classification Loss : 0.0072 | RPN Localization Loss : 0.0187 | FRCNN Classification Loss : 0.0415 | FRCNN Localization Loss : 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:28<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 24\n",
      "RPN Classification Loss : 0.0073 | RPN Localization Loss : 0.0187 | FRCNN Classification Loss : 0.0414 | FRCNN Localization Loss : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 25\n",
      "RPN Classification Loss : 0.0072 | RPN Localization Loss : 0.0187 | FRCNN Classification Loss : 0.0414 | FRCNN Localization Loss : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 26\n",
      "RPN Classification Loss : 0.0071 | RPN Localization Loss : 0.0186 | FRCNN Classification Loss : 0.0413 | FRCNN Localization Loss : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:28<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 27\n",
      "RPN Classification Loss : 0.0069 | RPN Localization Loss : 0.0185 | FRCNN Classification Loss : 0.0406 | FRCNN Localization Loss : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 28\n",
      "RPN Classification Loss : 0.0071 | RPN Localization Loss : 0.0185 | FRCNN Classification Loss : 0.0408 | FRCNN Localization Loss : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 29\n",
      "RPN Classification Loss : 0.0070 | RPN Localization Loss : 0.0184 | FRCNN Classification Loss : 0.0410 | FRCNN Localization Loss : 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:28<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 30\n",
      "RPN Classification Loss : 0.0070 | RPN Localization Loss : 0.0184 | FRCNN Classification Loss : 0.0404 | FRCNN Localization Loss : 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 31\n",
      "RPN Classification Loss : 0.0070 | RPN Localization Loss : 0.0183 | FRCNN Classification Loss : 0.0405 | FRCNN Localization Loss : 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:32<00:00, 27.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 32\n",
      "RPN Classification Loss : 0.0070 | RPN Localization Loss : 0.0183 | FRCNN Classification Loss : 0.0404 | FRCNN Localization Loss : 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:34<00:00, 27.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 33\n",
      "RPN Classification Loss : 0.0069 | RPN Localization Loss : 0.0183 | FRCNN Classification Loss : 0.0405 | FRCNN Localization Loss : 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:26<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 34\n",
      "RPN Classification Loss : 0.0068 | RPN Localization Loss : 0.0182 | FRCNN Classification Loss : 0.0401 | FRCNN Localization Loss : 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:29<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 35\n",
      "RPN Classification Loss : 0.0069 | RPN Localization Loss : 0.0181 | FRCNN Classification Loss : 0.0400 | FRCNN Localization Loss : 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 36\n",
      "RPN Classification Loss : 0.0068 | RPN Localization Loss : 0.0181 | FRCNN Classification Loss : 0.0398 | FRCNN Localization Loss : 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 37\n",
      "RPN Classification Loss : 0.0068 | RPN Localization Loss : 0.0180 | FRCNN Classification Loss : 0.0395 | FRCNN Localization Loss : 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 38\n",
      "RPN Classification Loss : 0.0067 | RPN Localization Loss : 0.0180 | FRCNN Classification Loss : 0.0398 | FRCNN Localization Loss : 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:27<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 39\n",
      "RPN Classification Loss : 0.0067 | RPN Localization Loss : 0.0180 | FRCNN Classification Loss : 0.0390 | FRCNN Localization Loss : 0.0159\n",
      "Train Completed!\n"
     ]
    }
   ],
   "source": [
    "backbone_resnet18_default = BackBoneResNet18()\n",
    "train_config['name_backbone'] = \"resnet18_default\"\n",
    "train(model_config, train_config, dataset_config, backbone_resnet18_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad953995",
   "metadata": {
    "papermill": {
     "duration": 4.690947,
     "end_time": "2024-12-25T19:20:06.825595",
     "exception": false,
     "start_time": "2024-12-25T19:20:02.134648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Thử với mô hình Backbone Retnet18, nhưng thêm 1 block với 2 lớp conv2d\n",
    "Kiến trúc này có khả năng làm giảm kích thước của feature map và tăng trường nhìn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a01e754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T19:20:16.413315Z",
     "iopub.status.busy": "2024-12-25T19:20:16.412862Z",
     "iopub.status.idle": "2024-12-25T19:20:16.423519Z",
     "shell.execute_reply": "2024-12-25T19:20:16.422718Z"
    },
    "papermill": {
     "duration": 4.85193,
     "end_time": "2024-12-25T19:20:16.424906",
     "exception": false,
     "start_time": "2024-12-25T19:20:11.572976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "    \n",
    "class BackBoneResNet18Add1Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        resnet18 = models.resnet18(weights=weights)\n",
    "\n",
    "        children = [child for child in resnet18.children()]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *children[:-2], ConvBlock()\n",
    "        )\n",
    "        # self.model = resnet18.features[:-1]\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out = self.model(X)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2293dcb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T19:20:26.184063Z",
     "iopub.status.busy": "2024-12-25T19:20:26.183777Z",
     "iopub.status.idle": "2024-12-25T22:24:41.756473Z",
     "shell.execute_reply": "2024-12-25T22:24:41.755277Z"
    },
    "papermill": {
     "duration": 11060.420404,
     "end_time": "2024-12-25T22:24:41.758060",
     "exception": false,
     "start_time": "2024-12-25T19:20:21.337656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "RPN Classification Loss : 0.1550 | RPN Localization Loss : 0.0558 | FRCNN Classification Loss : 0.3804 | FRCNN Localization Loss : 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1\n",
      "RPN Classification Loss : 0.0953 | RPN Localization Loss : 0.0462 | FRCNN Classification Loss : 0.2584 | FRCNN Localization Loss : 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2\n",
      "RPN Classification Loss : 0.0684 | RPN Localization Loss : 0.0407 | FRCNN Classification Loss : 0.1908 | FRCNN Localization Loss : 0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:34<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3\n",
      "RPN Classification Loss : 0.0501 | RPN Localization Loss : 0.0369 | FRCNN Classification Loss : 0.1525 | FRCNN Localization Loss : 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 4\n",
      "RPN Classification Loss : 0.0389 | RPN Localization Loss : 0.0339 | FRCNN Classification Loss : 0.1308 | FRCNN Localization Loss : 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 5\n",
      "RPN Classification Loss : 0.0318 | RPN Localization Loss : 0.0316 | FRCNN Classification Loss : 0.1209 | FRCNN Localization Loss : 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 6\n",
      "RPN Classification Loss : 0.0269 | RPN Localization Loss : 0.0298 | FRCNN Classification Loss : 0.1168 | FRCNN Localization Loss : 0.0254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 7\n",
      "RPN Classification Loss : 0.0231 | RPN Localization Loss : 0.0278 | FRCNN Classification Loss : 0.1117 | FRCNN Localization Loss : 0.0242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 8\n",
      "RPN Classification Loss : 0.0207 | RPN Localization Loss : 0.0261 | FRCNN Classification Loss : 0.1089 | FRCNN Localization Loss : 0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:37<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 9\n",
      "RPN Classification Loss : 0.0185 | RPN Localization Loss : 0.0249 | FRCNN Classification Loss : 0.1065 | FRCNN Localization Loss : 0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 10\n",
      "RPN Classification Loss : 0.0171 | RPN Localization Loss : 0.0240 | FRCNN Classification Loss : 0.1065 | FRCNN Localization Loss : 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 11\n",
      "RPN Classification Loss : 0.0168 | RPN Localization Loss : 0.0236 | FRCNN Classification Loss : 0.1054 | FRCNN Localization Loss : 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 12\n",
      "RPN Classification Loss : 0.0124 | RPN Localization Loss : 0.0194 | FRCNN Classification Loss : 0.0984 | FRCNN Localization Loss : 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 13\n",
      "RPN Classification Loss : 0.0103 | RPN Localization Loss : 0.0173 | FRCNN Classification Loss : 0.0943 | FRCNN Localization Loss : 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 14\n",
      "RPN Classification Loss : 0.0093 | RPN Localization Loss : 0.0163 | FRCNN Classification Loss : 0.0918 | FRCNN Localization Loss : 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 15\n",
      "RPN Classification Loss : 0.0087 | RPN Localization Loss : 0.0156 | FRCNN Classification Loss : 0.0899 | FRCNN Localization Loss : 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 16\n",
      "RPN Classification Loss : 0.0082 | RPN Localization Loss : 0.0150 | FRCNN Classification Loss : 0.0890 | FRCNN Localization Loss : 0.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 17\n",
      "RPN Classification Loss : 0.0081 | RPN Localization Loss : 0.0149 | FRCNN Classification Loss : 0.0887 | FRCNN Localization Loss : 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 18\n",
      "RPN Classification Loss : 0.0082 | RPN Localization Loss : 0.0148 | FRCNN Classification Loss : 0.0883 | FRCNN Localization Loss : 0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 19\n",
      "RPN Classification Loss : 0.0081 | RPN Localization Loss : 0.0148 | FRCNN Classification Loss : 0.0884 | FRCNN Localization Loss : 0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 20\n",
      "RPN Classification Loss : 0.0080 | RPN Localization Loss : 0.0148 | FRCNN Classification Loss : 0.0879 | FRCNN Localization Loss : 0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 21\n",
      "RPN Classification Loss : 0.0080 | RPN Localization Loss : 0.0147 | FRCNN Classification Loss : 0.0882 | FRCNN Localization Loss : 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 22\n",
      "RPN Classification Loss : 0.0080 | RPN Localization Loss : 0.0147 | FRCNN Classification Loss : 0.0883 | FRCNN Localization Loss : 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 23\n",
      "RPN Classification Loss : 0.0079 | RPN Localization Loss : 0.0146 | FRCNN Classification Loss : 0.0877 | FRCNN Localization Loss : 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 24\n",
      "RPN Classification Loss : 0.0078 | RPN Localization Loss : 0.0146 | FRCNN Classification Loss : 0.0879 | FRCNN Localization Loss : 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 25\n",
      "RPN Classification Loss : 0.0079 | RPN Localization Loss : 0.0145 | FRCNN Classification Loss : 0.0875 | FRCNN Localization Loss : 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 26\n",
      "RPN Classification Loss : 0.0078 | RPN Localization Loss : 0.0145 | FRCNN Classification Loss : 0.0879 | FRCNN Localization Loss : 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 27\n",
      "RPN Classification Loss : 0.0077 | RPN Localization Loss : 0.0145 | FRCNN Classification Loss : 0.0867 | FRCNN Localization Loss : 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 28\n",
      "RPN Classification Loss : 0.0077 | RPN Localization Loss : 0.0144 | FRCNN Classification Loss : 0.0872 | FRCNN Localization Loss : 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 29\n",
      "RPN Classification Loss : 0.0077 | RPN Localization Loss : 0.0144 | FRCNN Classification Loss : 0.0871 | FRCNN Localization Loss : 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 30\n",
      "RPN Classification Loss : 0.0077 | RPN Localization Loss : 0.0143 | FRCNN Classification Loss : 0.0869 | FRCNN Localization Loss : 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 31\n",
      "RPN Classification Loss : 0.0076 | RPN Localization Loss : 0.0143 | FRCNN Classification Loss : 0.0868 | FRCNN Localization Loss : 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 32\n",
      "RPN Classification Loss : 0.0075 | RPN Localization Loss : 0.0142 | FRCNN Classification Loss : 0.0864 | FRCNN Localization Loss : 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 33\n",
      "RPN Classification Loss : 0.0075 | RPN Localization Loss : 0.0142 | FRCNN Classification Loss : 0.0862 | FRCNN Localization Loss : 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 34\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0142 | FRCNN Classification Loss : 0.0864 | FRCNN Localization Loss : 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:36<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 35\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0141 | FRCNN Classification Loss : 0.0859 | FRCNN Localization Loss : 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:34<00:00, 27.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 36\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0141 | FRCNN Classification Loss : 0.0857 | FRCNN Localization Loss : 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 37\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0141 | FRCNN Classification Loss : 0.0862 | FRCNN Localization Loss : 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:35<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 38\n",
      "RPN Classification Loss : 0.0074 | RPN Localization Loss : 0.0140 | FRCNN Classification Loss : 0.0860 | FRCNN Localization Loss : 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [04:34<00:00, 27.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 39\n",
      "RPN Classification Loss : 0.0073 | RPN Localization Loss : 0.0140 | FRCNN Classification Loss : 0.0860 | FRCNN Localization Loss : 0.0150\n",
      "Train Completed!\n"
     ]
    }
   ],
   "source": [
    "backbone_resnet18_add1block = BackBoneResNet18Add1Block()\n",
    "train_config['name_backbone'] = 'resnet18_add1block'\n",
    "train(model_config, train_config, dataset_config, backbone_resnet18_add1block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19c864",
   "metadata": {
    "papermill": {
     "duration": 9.632154,
     "end_time": "2024-12-25T22:25:01.104460",
     "exception": false,
     "start_time": "2024-12-25T22:24:51.472306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121b7ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:25:20.429276Z",
     "iopub.status.busy": "2024-12-25T22:25:20.428921Z",
     "iopub.status.idle": "2024-12-25T22:25:20.443324Z",
     "shell.execute_reply": "2024-12-25T22:25:20.442534Z"
    },
    "papermill": {
     "duration": 9.713871,
     "end_time": "2024-12-25T22:25:20.444592",
     "exception": false,
     "start_time": "2024-12-25T22:25:10.730721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "def infer(args):\n",
    "    if not os.path.exists(f\"samples/{args['name_model']}\"):\n",
    "        os.makedirs(f\"samples/{args['name_model']}\")\n",
    "    faster_rcnn_model = args['model']\n",
    "    test_dataset = args['test_dataset']\n",
    "    \n",
    "    faster_rcnn_model.roi_head.low_score_threshold = 0.7\n",
    "    \n",
    "    for sample_count in tqdm(range(len(test_dataset))):\n",
    "        im, target, frame = test_dataset[sample_count]\n",
    "        im = im.unsqueeze(0).float().to(device)\n",
    "        frame = frame.cpu().numpy()\n",
    "        # print(frame)\n",
    "\n",
    "        gt_im = frame.copy()\n",
    "        gt_im_copy = gt_im.copy()\n",
    "        \n",
    "        # Saving images with ground truth boxes\n",
    "        for idx, box in enumerate(target['bboxes']):\n",
    "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            \n",
    "            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
    "            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])\n",
    "            text = test_dataset.idx2label[target['labels'][idx].detach().cpu().item()]\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
    "            text_w, text_h = text_size\n",
    "            cv2.rectangle(gt_im_copy , (x1, y1), (x1 + 10+text_w, y1 + 10+text_h), [255, 255, 255], -1)\n",
    "            cv2.putText(gt_im, text=test_dataset.idx2label[target['labels'][idx].detach().cpu().item()],\n",
    "                        org=(x1+5, y1+15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "            cv2.putText(gt_im_copy, text=text,\n",
    "                        org=(x1 + 5, y1 + 15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)\n",
    "        path_gt = os.path.join('sample', f\"{args['name_model']}\", 'gt', 'output_frcnn_gt_{}.png'.format(sample_count))\n",
    "        dir_gt = os.path.dirname(path_gt)\n",
    "        if not os.path.exists(dir_gt):\n",
    "            os.makedirs(dir_gt, exist_ok=True)\n",
    "\n",
    "        cv2.imwrite(path_gt, gt_im)\n",
    "        \n",
    "        # Getting predictions from trained model\n",
    "        rpn_output, frcnn_output = faster_rcnn_model(im, None)\n",
    "        boxes = frcnn_output['boxes']\n",
    "        labels = frcnn_output['labels']\n",
    "        scores = frcnn_output['scores']\n",
    "        im = frame.copy()\n",
    "        im_copy = im.copy()\n",
    "        \n",
    "        # Saving images with predicted boxes\n",
    "        for idx, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
    "            cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])\n",
    "            text = '{} : {:.2f}'.format(test_dataset.idx2label[labels[idx].detach().cpu().item()],\n",
    "                                        scores[idx].detach().cpu().item())\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)\n",
    "            text_w, text_h = text_size\n",
    "            cv2.rectangle(im_copy , (x1, y1), (x1 + 10+text_w, y1 + 10+text_h), [255, 255, 255], -1)\n",
    "            cv2.putText(im, text=text,\n",
    "                        org=(x1+5, y1+15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "            cv2.putText(im_copy, text=text,\n",
    "                        org=(x1 + 5, y1 + 15),\n",
    "                        thickness=1,\n",
    "                        fontScale=1,\n",
    "                        color=[0, 0, 0],\n",
    "                        fontFace=cv2.FONT_HERSHEY_PLAIN)\n",
    "        cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)\n",
    "        path_pred = os.path.join('sample', f\"{args['name_model']}\", 'pred', 'output_frcnn_pred_{}.png'.format(sample_count))\n",
    "        dir_pred = os.path.dirname(path_pred)\n",
    "        if not os.path.exists(dir_pred):\n",
    "            os.makedirs(dir_pred, exist_ok=True)\n",
    "        cv2.imwrite(path_pred.format(sample_count), im)\n",
    "\n",
    "# infer(cfg_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02340fe",
   "metadata": {
    "papermill": {
     "duration": 9.606763,
     "end_time": "2024-12-25T22:25:39.876502",
     "exception": false,
     "start_time": "2024-12-25T22:25:30.269739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Chạy infer, kết quả các ảnh được lưu ở output sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10e0e779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:25:59.395881Z",
     "iopub.status.busy": "2024-12-25T22:25:59.395558Z",
     "iopub.status.idle": "2024-12-25T22:26:04.291180Z",
     "shell.execute_reply": "2024-12-25T22:26:04.290195Z"
    },
    "papermill": {
     "duration": 14.762857,
     "end_time": "2024-12-25T22:26:04.292693",
     "exception": false,
     "start_time": "2024-12-25T22:25:49.529836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 10.52it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_infer(path_model, backbone, name_model):\n",
    "    # name_faster_rcnn_model_backbone_resnet18 = '/kaggle/input/trained-model-obj-detection/faster_rcnn_fruit_detection.pth'\n",
    "    faster_rcnn_model= FasterRCNN(\n",
    "        model_config, \n",
    "        num_classes=dataset_config['num_classes'],\n",
    "        backbone=backbone\n",
    "    )\n",
    "    faster_rcnn_model.load_state_dict(torch.load(path_model, weights_only=True, map_location=device))\n",
    "    faster_rcnn_model.to(device)\n",
    "    faster_rcnn_model.eval()\n",
    "    cfg_infer_faster_rcnn_model = {\n",
    "        'model': faster_rcnn_model,\n",
    "        'test_dataset': test_dataset,\n",
    "        'name_model': name_model\n",
    "    }\n",
    "    infer(cfg_infer_faster_rcnn_model)\n",
    "    \n",
    "name_faster_rcnn_model_backbone_resnet18 = '/kaggle/working/fruit_detection/resnet18_default/faster_rcnn_fruit_detection.pth'\n",
    "name_faster_rcnn_model_backbone_resnet18_add1block = '/kaggle/working/fruit_detection/resnet18_add1block/faster_rcnn_fruit_detection.pth'\n",
    "\n",
    "backbone_noaddblock = BackBoneResNet18()\n",
    "backbone_add1block = BackBoneResNet18Add1Block()\n",
    "\n",
    "name_model_noaddblock = 'noaddblock'\n",
    "name_model_add1block = 'add1block'\n",
    "\n",
    "run_infer(name_faster_rcnn_model_backbone_resnet18, backbone_noaddblock, name_model_noaddblock)\n",
    "run_infer(name_faster_rcnn_model_backbone_resnet18_add1block, backbone_add1block, name_model_add1block)\n",
    "print('Infer completed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc417a",
   "metadata": {
    "papermill": {
     "duration": 9.666128,
     "end_time": "2024-12-25T22:26:23.541780",
     "exception": false,
     "start_time": "2024-12-25T22:26:13.875652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lưu kết quả khi thực hiện infer trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23bc6472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:26:42.947831Z",
     "iopub.status.busy": "2024-12-25T22:26:42.947531Z",
     "iopub.status.idle": "2024-12-25T22:26:43.545285Z",
     "shell.execute_reply": "2024-12-25T22:26:43.544589Z"
    },
    "papermill": {
     "duration": 10.435766,
     "end_time": "2024-12-25T22:26:43.546778",
     "exception": false,
     "start_time": "2024-12-25T22:26:33.111012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def create_and_save_results_infering_with_test(model, name_model):\n",
    "    valid_extensions_image = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\"}\n",
    "    folder_contain_images_path = '/kaggle/input/k-67-object-detection/Dataset/Test/images'\n",
    "    image_paths = get_valid_file_in_folder(folder_contain_images_path, valid_extensions_image)\n",
    "    model.roi_head.low_score_threshold = 0.7\n",
    "\n",
    "    results = {\n",
    "        'ID': [],\n",
    "        'bounding_boxes': []\n",
    "    }\n",
    "\n",
    "    for image_path in tqdm(image_paths):\n",
    "        image = Image.open(image_path)\n",
    "        im_tensor = torchvision.transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    "\n",
    "        rpn_output, frcnn_output = model(im_tensor, None)\n",
    "        boxes = frcnn_output['boxes']\n",
    "        labels = frcnn_output['labels']\n",
    "        scores = frcnn_output['scores']\n",
    "\n",
    "        preds = []\n",
    "        \n",
    "        for idx, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = box.detach().cpu().numpy()\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            label = labels[idx].detach().cpu().item()\n",
    "            score = scores[idx].detach().cpu().item()\n",
    "\n",
    "            preds.append({\n",
    "                'x_min': x1, 'y_min': y1, 'x_max': x2, 'y_max': y2,\n",
    "                'class': label,\n",
    "                'confidence': score\n",
    "            })\n",
    "        \n",
    "        file_name_image = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        results['ID'].append(file_name_image)\n",
    "        results['bounding_boxes'].append(json.dumps(preds))\n",
    "\n",
    "    path_save = f\"submit/{name_model}/results_objects_detection.csv\"\n",
    "    dir_save = os.path.dirname(path_save)\n",
    "    if not os.path.exists(dir_save):\n",
    "        os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Lưu vào file CSV\n",
    "    df.to_csv(path_save, index=False)\n",
    "    print('Save completed!')\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e59c919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T22:27:03.030094Z",
     "iopub.status.busy": "2024-12-25T22:27:03.029447Z",
     "iopub.status.idle": "2024-12-25T22:28:05.120916Z",
     "shell.execute_reply": "2024-12-25T22:28:05.120011Z"
    },
    "papermill": {
     "duration": 79.547574,
     "end_time": "2024-12-25T22:28:12.949066",
     "exception": false,
     "start_time": "2024-12-25T22:26:53.401492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 848/848 [00:34<00:00, 24.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 848/848 [00:26<00:00, 31.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_save_csv(path_model, backbone, name_model):\n",
    "    faster_rcnn_model= FasterRCNN(\n",
    "        model_config, \n",
    "        num_classes=dataset_config['num_classes'],\n",
    "        backbone=backbone\n",
    "    )\n",
    "    faster_rcnn_model.load_state_dict(torch.load(path_model, weights_only=True, map_location=device))\n",
    "    faster_rcnn_model.to(device)\n",
    "    faster_rcnn_model.eval()\n",
    "\n",
    "    create_and_save_results_infering_with_test(faster_rcnn_model, name_model)\n",
    "\n",
    "run_save_csv(name_faster_rcnn_model_backbone_resnet18, backbone_noaddblock, name_model_noaddblock)\n",
    "run_save_csv(name_faster_rcnn_model_backbone_resnet18_add1block, backbone_add1block, name_model_add1block)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10485801,
     "sourceId": 90356,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22058.829936,
   "end_time": "2024-12-25T22:28:24.356963",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-25T16:20:45.527027",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
